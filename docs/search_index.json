[["index.html", "The Data Analyst’s Guide to Cause and Effect Companion website Welcome! Version Errata", " The Data Analyst’s Guide to Cause and Effect Companion website Theiss Bendixen &amp; Benjamin Grant Purzycki 2025-08-24 Welcome! The is the companion website for The Data Analyst’s Guide to Cause and Effect (Bendixen &amp; Purzycki, 2026). Version This is version 1.0 Errata While we have made every effort to ensure the accuracy and currency of both the book and this website, some errors may have inevitably slipped through. In this section, we will document important errors or updates as they are brought to our attention. "],["chapter-1.html", "Chapter 1 Introduction", " Chapter 1 Introduction No supplementary content for Chapter 1. "],["chapter-2.html", "Chapter 2 Causal Graphs 2.1 The Fork 2.2 The Pipe 2.3 The Collider 2.4 Post-treatment bias 2.5 Session info", " Chapter 2 Causal Graphs 2.1 The Fork We first simulate some data from the simple ‘Fork’ DAG. set.seed(1747) n &lt;- 1e4 bZ &lt;- 1 Z &lt;- rnorm(n, 0, 1) X &lt;- Z*bZ + rnorm(n, 0, 1) Y &lt;- Z*bZ + rnorm(n, 0, 1) We then write a function for fitting and plotting our models that we can re-use for the ‘Pipe’ and ‘Collider’ scenarios. This function depends on the ggplot2 (Wickham et al. 2019) and patchwork (Pedersen 2024) packages. library(ggplot2) library(patchwork) plot_scat &lt;- function(data, title){ # Y ~ X p1 &lt;- ggplot(data, aes(x=X, y=Y)) + geom_point(alpha = 0.05, size = .1) + geom_smooth(method=&#39;lm&#39;, color = &quot;blue&quot;, linewidth = 0.5) + theme_classic() + labs(title = title, subtitle = &quot;Y ~ X&quot;) # Y ~ X + Z model &lt;- lm(Y ~ X + Z, data = data) new_data &lt;- transform(data, Z = 0) predictions &lt;- predict(model, newdata = new_data, interval = &quot;confidence&quot;) p2 &lt;- ggplot(data, aes(x = X, y = Y)) + geom_point(alpha = 0.05, size = .1) + geom_line(data = new_data, aes(y = predictions[, &quot;fit&quot;]), linewidth = 0.5, color = &quot;blue&quot;) + geom_ribbon(data = new_data, aes(ymin = predictions[, &quot;lwr&quot;], ymax = predictions[, &quot;upr&quot;]), alpha = 0.2) + theme_classic() + labs(subtitle = &quot;Y ~ X + Z&quot;) return(p1 + p2) } We then apply the function to the simulated data. plot_scat(data = data.frame(Y=Y, X=X, Z=Z), title = &quot;The Fork&quot;) 2.2 The Pipe Similarly to above, we simulate data from the ‘Pipe’ DAG… set.seed(1747) n &lt;- 1e4 bZ &lt;- 1 bX &lt;- 1 X &lt;- rnorm(n, 0, 1) Z &lt;- X*bX + rnorm(n, 0, 1) Y &lt;- Z*bZ + rnorm(n, 0, 1) … and then apply our custom fitting and plotting function. plot_scat(data = data.frame(Y=Y, X=X, Z=Z), title = &quot;The Pipe&quot;) 2.3 The Collider Exactly the same approach as above. set.seed(1747) n &lt;- 1e4 bX &lt;- 1 bY &lt;- 1 X &lt;- rnorm(n, 0, 1) Y &lt;- rnorm(n, 0, 1) Z &lt;- X*bX + Y*bY + rnorm(n, 0, 1) plot_scat(data = data.frame(Y=Y, X=X, Z=Z), title = &quot;The Collider&quot;) 2.4 Post-treatment bias Again, following the same approach, we simulate data from the post-treatment DAG. set.seed(1747) n &lt;- 1e4 bZ &lt;- 1 bX &lt;- 0.5 bY &lt;- 1 Z &lt;- rnorm(n, 0, 1) X &lt;- Z*bZ + rnorm(n, 0, 1) Y &lt;- Z*bZ + X*bX + rnorm(n, 0, 1) P &lt;- Y*bY + rnorm(n, 0, 1) Next, we fit two models: One that adjusts for the post-treatment variable P… glm(Y ~ X + Z + P, data = data.frame(Y=Y, X=X, Z=Z, P=P)) ## ## Call: glm(formula = Y ~ X + Z + P, data = data.frame(Y = Y, X = X, ## Z = Z, P = P)) ## ## Coefficients: ## (Intercept) X Z P ## 0.01018 0.25354 0.50133 0.48813 ## ## Degrees of Freedom: 9999 Total (i.e. Null); 9996 Residual ## Null Deviance: 34040 ## Residual Deviance: 4895 AIC: 21240 … and another that doesn’t. glm(Y ~ X + Z, data = data.frame(Y=Y, X=X, Z=Z)) ## ## Call: glm(formula = Y ~ X + Z, data = data.frame(Y = Y, X = X, Z = Z)) ## ## Coefficients: ## (Intercept) X Z ## 0.007601 0.495594 0.987202 ## ## Degrees of Freedom: 9999 Total (i.e. Null); 9997 Residual ## Null Deviance: 34040 ## Residual Deviance: 9793 AIC: 28180 We see that only the latter model picks up the correct estimate for X, which was 0.5 in this case. 2.5 Session info sessionInfo() ## R version 4.4.1 (2024-06-14 ucrt) ## Platform: x86_64-w64-mingw32/x64 ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=Danish_Denmark.utf8 LC_CTYPE=Danish_Denmark.utf8 ## [3] LC_MONETARY=Danish_Denmark.utf8 LC_NUMERIC=C ## [5] LC_TIME=Danish_Denmark.utf8 ## ## time zone: Europe/Copenhagen ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] patchwork_1.3.0 ggplot2_3.5.1 ## ## loaded via a namespace (and not attached): ## [1] Matrix_1.7-0 gtable_0.3.5 jsonlite_1.8.8 dplyr_1.1.4 ## [5] compiler_4.4.1 highr_0.11 tidyselect_1.2.1 jquerylib_0.1.4 ## [9] splines_4.4.1 scales_1.3.0 yaml_2.3.8 fastmap_1.2.0 ## [13] lattice_0.22-6 R6_2.5.1 labeling_0.4.3 generics_0.1.4 ## [17] knitr_1.47 tibble_3.2.1 bookdown_0.41 munsell_0.5.1 ## [21] bslib_0.7.0 pillar_1.10.2 rlang_1.1.4 cachem_1.1.0 ## [25] xfun_0.48 sass_0.4.9 cli_3.6.2 withr_3.0.2 ## [29] magrittr_2.0.3 mgcv_1.9-1 digest_0.6.35 grid_4.4.1 ## [33] rstudioapi_0.16.0 lifecycle_1.0.4 nlme_3.1-164 vctrs_0.6.5 ## [37] evaluate_1.0.4 glue_1.7.0 farver_2.1.2 colorspace_2.1-0 ## [41] rmarkdown_2.27 tools_4.4.1 pkgconfig_2.0.3 htmltools_0.5.8.1 References "],["chapter-3.html", "Chapter 3 G-methods and Marginal Effects 3.1 Inverse probability weighting 3.2 G-computation 3.3 Session info", " Chapter 3 G-methods and Marginal Effects 3.1 Inverse probability weighting For our illustration of IPW and g-computation, we simulate some simple confounded data. set.seed(1747) n &lt;- 1e4 bZ &lt;- 2 bX &lt;- 2 Z &lt;- rnorm(n, 0, 0.5) X &lt;- rbinom(n, 1, plogis(0.5 + Z*bZ)) Y &lt;- rnorm(n, 10 + X*bX + Z*bZ) d &lt;- data.frame(Y=Y, X=X, Z=Z) For IPW, we first fit a logistic regression model of the probability of receiving treatment. treatment_model &lt;- glm(X ~ Z, data = d, family = &quot;binomial&quot;) We then predict for each individual their probability of receiving treatment. d$pX &lt;- predict(treatment_model, type = &quot;response&quot;) Lastly, we inverse those probabilities and use them as weights in a model – a so-called ‘marginal structural model’ – that regresses Y on X. d$w &lt;- with(d, ifelse(X==1, 1/pX, 1/(1-pX))) lm(Y ~ X, data = d, weights = w) We then compare the treatment (solid lines) and control (dashed lines) groups before (i.e., in the observed sample) and after weighting (i.e., the IPW ‘pseudo-population’). # IPW with unstabilized weights library(ggplot2) library(patchwork) p1 &lt;- ggplot() + # X = 1 (sample) geom_density(data = subset(d, X == 1), aes(x = pX), size = 1) + # X = 0 (sample) geom_density(data = subset(d, X == 0), aes(x = pX), linetype = &quot;dashed&quot;, size = 1) + theme_classic() + theme( axis.text.y = element_blank(), axis.ticks.y = element_blank(), axis.title.y = element_blank(), axis.line.y = element_blank()) + xlim(c(0,1)) + xlab(&quot;Probability of treatment&quot;) + ggtitle(&quot;Before IP weighting&quot;) p2 &lt;- ggplot() + # X = 1 (pseudo-population) geom_density(data = subset(d, X == 1), aes(x = pX, weight = w), size = 1) + # X = 0 (pseudo-population) geom_density(data = subset(d, X == 0), aes(x = pX, weight = w), linetype = &quot;dashed&quot;, size = 1) + theme_classic() + theme( axis.text.y = element_blank(), axis.ticks.y = element_blank(), axis.title.y = element_blank(), axis.line.y = element_blank()) + xlim(c(0,1)) + xlab(&quot;Probability of treatment&quot;) + ggtitle(&quot;After IP weighting&quot;) (p1 + p2) The above approach showcases IPW with so-called ‘unstabilized’ weights. But stabilizing the IP weights are often recommended. Stabilized weights uses an unconditional model for the treatment probability instead of 1 as the numerator in the IPW formula. Let’s visualize this wit the stabilized weights plotted with a dashed curve. We see that the stabilized weights are much less extreme. # IPW with stabilized weights pn &lt;- glm(X ~ 1, data = d) d$pnX &lt;- predict(pn, type = &quot;response&quot;) d$sw &lt;- with(d, ifelse(X==1, pnX/pX, (1-pnX)/(1-pX))) p3 &lt;- ggplot() + geom_density(data = d, aes(x = w), size = 1) + geom_density(data = d, aes(x = sw), linetype = &quot;dashed&quot;, size = 1) + theme_classic() + theme_classic() + theme( axis.text.y = element_blank(), axis.ticks.y = element_blank(), axis.title.y = element_blank(), axis.line.y = element_blank()) + xlab(&quot;IP weight&quot;) + coord_cartesian(xlim = c(0,10)) + labs(title = &quot;Unstabilized and stabilized weights&quot;) p3 3.1.1 Bootstrapping Here’s a basic bootstrapping approach for IPW. We use only 100 bootstrap samples, but in practice we’d often want (many) more. # Load necessary libraries library(boot) # Number of bootstrap samples n_bootstrap &lt;- 100 # Function to perform the analysis on a bootstrapped sample bootstrap_analysis &lt;- function(data, indices) { # Resample the data d &lt;- data[indices, ] # Fit the treatment model using logistic regression treatment_model &lt;- glm(X ~ Z, data = d, family = &quot;binomial&quot;) # Calculate predicted probabilities d$pX &lt;- predict(treatment_model, type = &quot;response&quot;) # Calculate weights d$w &lt;- with(d, ifelse(X == 1, 1 / pX, 1 / (1 - pX))) # Fit the weighted linear regression model weighted_model &lt;- lm(Y ~ X, data = d, weights = w) # Return the coefficient of X return(coef(weighted_model)[&quot;X&quot;]) } # Perform bootstrapping bootstrap_results &lt;- boot(data = d, statistic = bootstrap_analysis, R = n_bootstrap) # Summarize the bootstrap results bootstrap_summary &lt;- boot.ci(bootstrap_results, type = &quot;norm&quot;) # Print the results print(bootstrap_summary) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 100 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = bootstrap_results, type = &quot;norm&quot;) ## ## Intervals : ## Level Normal ## 95% ( 1.946, 2.057 ) ## Calculations and Intervals on Original Scale 3.1.2 ‘Robust’ standard errors for IPW library(sandwich) # robust standard errors for coefficients fit &lt;- lm(Y ~ X, data = d, weights = w) vcovHC(fit, type = &#39;HC0&#39;) ## (Intercept) X ## (Intercept) 0.001036983 -0.001036983 ## X -0.001036983 0.001564404 3.2 G-computation Here, we show a basic g-computation workflow… model &lt;- lm(Y ~ X + Z, data = d) d$EX1 &lt;- predict(model, newdata = transform(d, X = 1)) d$EX0 &lt;- predict(model, newdata = transform(d, X = 0)) with(d, mean(EX1)-mean(EX0)) … And code to produce the table showing both observed and predicted values, some of which are counter-factual. vars &lt;- c(&quot;Y&quot;, &quot;X&quot;, &quot;Z&quot;, &quot;EX1&quot;, &quot;EX0&quot;) xtable::xtable(head(d[vars]), digits = c(0,1,0,1,1,1)) ## % latex table generated in R 4.4.1 by xtable 1.8-4 package ## % Sun Aug 24 22:05:56 2025 ## \\begin{table}[ht] ## \\centering ## \\begin{tabular}{rrrrrr} ## \\hline ## &amp; Y &amp; X &amp; Z &amp; EX1 &amp; EX0 \\\\ ## \\hline ## 1 &amp; 15.3 &amp; 1 &amp; 0.6 &amp; 13.2 &amp; 11.2 \\\\ ## 2 &amp; 11.1 &amp; 0 &amp; 0.1 &amp; 12.2 &amp; 10.2 \\\\ ## 3 &amp; 10.6 &amp; 1 &amp; -0.1 &amp; 11.7 &amp; 9.7 \\\\ ## 4 &amp; 12.9 &amp; 1 &amp; 0.7 &amp; 13.4 &amp; 11.3 \\\\ ## 5 &amp; 11.0 &amp; 1 &amp; -0.5 &amp; 10.9 &amp; 8.9 \\\\ ## 6 &amp; 9.4 &amp; 0 &amp; 0.5 &amp; 12.9 &amp; 10.9 \\\\ ## \\hline ## \\end{tabular} ## \\end{table} 3.2.1 Bootstrapping Next, we show a basic bootstrapped g-computation implementation, again using only 100 bootstrap samples to ease the computational burden of the example. # Number of bootstrap samples n_bootstrap &lt;- 100 # Define the function to perform the analysis on a bootstrapped sample bootstrap_analysis &lt;- function(data, indices) { # Resample the data d &lt;- data[indices, ] # Fit the linear regression model model &lt;- lm(Y ~ X + Z, data = d) # Calculate predicted values for X = 1 and X = 0 d$EX1 &lt;- predict(model, newdata = transform(d, X = 1)) d$EX0 &lt;- predict(model, newdata = transform(d, X = 0)) # Compute the difference in means ate &lt;- with(d, mean(EX1) - mean(EX0)) return(ate) } # Perform bootstrapping bootstrap_results &lt;- boot(data = d, statistic = bootstrap_analysis, R = n_bootstrap) # Summarize the bootstrap results bootstrap_summary &lt;- boot.ci(bootstrap_results, type = c(&quot;norm&quot;)) # Print the results print(bootstrap_summary) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 100 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = bootstrap_results, type = c(&quot;norm&quot;)) ## ## Intervals : ## Level Normal ## 95% ( 1.962, 2.050 ) ## Calculations and Intervals on Original Scale 3.2.2 Bayesian g-computation Lastly, we show a Bayesian g-computation workflow using the R package brms (Bürkner 2017, 2018, 2021), which requires RStan (Stan Development Team, n.d.), for model fitting and tidybayes for post-processing (Kay 2023). library(brms) library(tidybayes) library(dplyr) # Fit Bayesian regression bmodel &lt;- brm(Y ~ X + Z, data = d, cores = 4, seed = 1, file = &quot;fits/bmodel.rds&quot;) # Calculate predicted values for X = 1 and X = 0 bEX1 &lt;- add_epred_draws(object = bmodel, newdata = transform(d, X = 1)) bEX0 &lt;- add_epred_draws(object = bmodel, newdata = transform(d, X = 0)) The key thing to note when working with Bayesian model fits is that we need to calculate our quantity of interest (here, the ATE) within each posterior draw. # Compute the difference in means ate &lt;- data.frame(EX1 = bEX1$.epred, EX0 = bEX0$.epred, draw = bEX0$.draw) |&gt; # For each posterior draw... group_by(draw) |&gt; # ... Calculate ATE summarise(ate = mean(EX1 - EX0)) We can summarize the posterior ATE by its mean and highest posterior density interval. mean_hdi(ate$ate) ## y ymin ymax .width .point .interval ## 1 2.009257 1.964355 2.051421 0.95 mean hdi An alternative approach – when we have a fitted model, Bayesian or otherwise – is via the versatile and very well documented marginaleffects package (Arel-Bundock, Greifer, and Heiss Forthcoming). library(marginaleffects) avg_comparisons(bmodel, variables = list(X = 0:1)) ## ## Estimate 2.5 % 97.5 % ## 2.01 1.96 2.05 ## ## Term: X ## Type: response ## Comparison: mean(1) - mean(0) ## Columns: term, contrast, estimate, conf.low, conf.high, predicted_lo, predicted_hi, predicted, tmp_idx 3.3 Session info sessionInfo() ## R version 4.4.1 (2024-06-14 ucrt) ## Platform: x86_64-w64-mingw32/x64 ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=Danish_Denmark.utf8 LC_CTYPE=Danish_Denmark.utf8 ## [3] LC_MONETARY=Danish_Denmark.utf8 LC_NUMERIC=C ## [5] LC_TIME=Danish_Denmark.utf8 ## ## time zone: Europe/Copenhagen ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] marginaleffects_0.23.0 dplyr_1.1.4 tidybayes_3.0.6 ## [4] brms_2.21.0 Rcpp_1.0.12 sandwich_3.1-1 ## [7] boot_1.3-30 patchwork_1.3.0 ggplot2_3.5.1 ## ## loaded via a namespace (and not attached): ## [1] gtable_0.3.5 tensorA_0.36.2.1 xfun_0.48 ## [4] bslib_0.7.0 QuickJSR_1.2.2 insight_0.20.5 ## [7] collapse_2.0.16 inline_0.3.19 lattice_0.22-6 ## [10] vctrs_0.6.5 tools_4.4.1 generics_0.1.4 ## [13] stats4_4.4.1 parallel_4.4.1 tibble_3.2.1 ## [16] highr_0.11 pkgconfig_2.0.3 Matrix_1.7-0 ## [19] data.table_1.15.4 checkmate_2.3.1 distributional_0.5.0 ## [22] RcppParallel_5.1.7 lifecycle_1.0.4 compiler_4.4.1 ## [25] farver_2.1.2 stringr_1.5.1 Brobdingnag_1.2-9 ## [28] munsell_0.5.1 codetools_0.2-20 htmltools_0.5.8.1 ## [31] sass_0.4.9 bayesplot_1.11.1 yaml_2.3.8 ## [34] tidyr_1.3.1 pillar_1.10.2 jquerylib_0.1.4 ## [37] arrayhelpers_1.1-0 cachem_1.1.0 StanHeaders_2.35.0.9000 ## [40] bridgesampling_1.1-2 abind_1.4-8 nlme_3.1-164 ## [43] posterior_1.6.1 rstan_2.35.0.9000 svUnit_1.0.6 ## [46] tidyselect_1.2.1 digest_0.6.35 mvtnorm_1.2-5 ## [49] stringi_1.8.4 purrr_1.0.2 bookdown_0.41 ## [52] labeling_0.4.3 splines_4.4.1 fastmap_1.2.0 ## [55] grid_4.4.1 colorspace_2.1-0 cli_3.6.2 ## [58] magrittr_2.0.3 loo_2.8.0 pkgbuild_1.4.4 ## [61] withr_3.0.2 scales_1.3.0 backports_1.5.0 ## [64] estimability_1.5.1 rmarkdown_2.27 matrixStats_1.3.0 ## [67] emmeans_1.10.5 gridExtra_2.3 zoo_1.8-12 ## [70] coda_0.19-4.1 evaluate_1.0.4 knitr_1.47 ## [73] ggdist_3.3.2 mgcv_1.9-1 rstantools_2.4.0 ## [76] rlang_1.1.4 xtable_1.8-4 glue_1.7.0 ## [79] rstudioapi_0.16.0 jsonlite_1.8.8 R6_2.5.1 References "],["chapter-4.html", "Chapter 4 Adventures in G-methods 4.1 Doubly robust estimation 4.2 Bootstrapped sub-group analysis 4.3 Complex longitudinal exposure-outcome feedback 4.4 Session info", " Chapter 4 Adventures in G-methods 4.1 Doubly robust estimation For demonstrating a ‘doubly robust’ estimator that combines IPW and g-computation, we use the nhefs data from the causaldata package (Huntington-Klein and Barrett 2024). This data come from the National Health and Nutrition Examination Survey Data I Epidemiologic Follow-up Study. library(causaldata) d &lt;- nhefs We first calculate stabilized IP weights. treat_mod &lt;- glm(qsmk ~ sex + age, data = d, family = &quot;binomial&quot;) d$pX &lt;- predict(treat_mod, type = &quot;response&quot;) pn &lt;- glm(qsmk ~ 1, data = d, family = &quot;binomial&quot;) d$pnX &lt;- predict(pn, type = &quot;response&quot;) d$sw &lt;- with(d, ifelse(qsmk==1, pnX/pX, (1-pnX)/(1-pX))) We can then plot the sample before and after weighting. library(ggplot2) library(patchwork) p1 &lt;- ggplot() + # X = 1 (sample) geom_density(data = subset(d, qsmk == 1), aes(x = pX), size = 1) + # X = 0 (sample) geom_density(data = subset(d, qsmk == 0), aes(x = pX), linetype = &quot;dashed&quot;, size = 1) + theme_classic() + theme( axis.text.y = element_blank(), axis.ticks.y = element_blank(), axis.title.y = element_blank(), axis.line.y = element_blank()) + xlim(c(0,1)) + xlab(&quot;Probability of treatment&quot;) + ggtitle(&quot;Before IP weighting&quot;) p2 &lt;- ggplot() + # X = 1 (pseudo-population) geom_density(data = subset(d, qsmk == 1), aes(x = pX, weight = sw), size = 1) + # X = 0 (pseudo-population) geom_density(data = subset(d, qsmk == 0), aes(x = pX, weight = sw), linetype = &quot;dashed&quot;, size = 1) + theme_classic() + theme( axis.text.y = element_blank(), axis.ticks.y = element_blank(), axis.title.y = element_blank(), axis.line.y = element_blank()) + xlim(c(0,1)) + xlab(&quot;Probability of treatment&quot;) + ggtitle(&quot;After IP weighting&quot;) (p1 + p2) We can also make a ‘love plot’ using the cobalt package (Greifer 2024) to inspect whether the IP weights ensures acceptable balance on the level of individual covariates. By setting continuous = \"std\", we indicate that the function should return the standardized absolute mean difference for any continuous variables (here, age). If we wanted the raw absolute mean difference, we’d set continuous = \"raw\". library(cobalt) love.plot(treat_mod, abs = TRUE, sample.names = c(&quot;Unweighted&quot;, &quot;IP Weighted&quot;), weights = d$sw, colors = c(&quot;grey60&quot;, &quot;black&quot;), thresholds = c(m = .1)) bal.tab(treat_mod, abs = TRUE, un = TRUE, thresholds = c(m = .1), weights = d$sw, continuous = &quot;std&quot;)$Balance Finally, we include the stabilized weights in an outcome model, which we in turn use for g-computation. out_mod &lt;- lm(wt82_71 ~ qsmk + sex + age, data = d, weights = sw) EX1 &lt;- predict(out_mod, newdata = transform(d, qsmk = 1)) EX0 &lt;- predict(out_mod, newdata = transform(d, qsmk = 0)) mean(EX1)-mean(EX0) ## [1] 3.039286 4.1.1 Bootstrapping The basic approach to bootstrapping is similar as in the previous chapter. Here, we bootstrap the doubly robust estimator from above. We use only 100 bootstrap samples, but in practice we’d often want more. library(boot) # Number of bootstrap samples n_bootstrap &lt;- 100 bootstrap_analysis &lt;- function(data, indices) { # Resample the data d &lt;- data[indices, ] # IPW treat_mod &lt;- glm(qsmk ~ sex + age, data = d, family = &quot;binomial&quot;) d$pX &lt;- predict(treat_mod, type = &quot;response&quot;) pn &lt;- glm(qsmk ~ 1, data = d, family = &quot;binomial&quot;) d$pnX &lt;- predict(pn, type = &quot;response&quot;) d$sw &lt;- with(d, ifelse(qsmk==1, pnX/pX, (1-pnX)/(1-pX))) # G-computation with IP weighted outcome model out_mod &lt;- lm(wt82_71 ~ qsmk + sex + age, data = d, weights = sw) EX1 &lt;- predict(out_mod, newdata = transform(d, qsmk = 1)) EX0 &lt;- predict(out_mod, newdata = transform(d, qsmk = 0)) mean(EX1)-mean(EX0) # Return the coefficient of X return(mean(EX1)-mean(EX0)) } # Perform bootstrapping bootstrap_results &lt;- boot(data = d, statistic = bootstrap_analysis, R = n_bootstrap) # Summarize the bootstrap results bootstrap_summary &lt;- boot.ci(bootstrap_results, type = &quot;norm&quot;) # Print the results print(bootstrap_summary) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 100 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = bootstrap_results, type = &quot;norm&quot;) ## ## Intervals : ## Level Normal ## 95% ( 2.092, 3.909 ) ## Calculations and Intervals on Original Scale 4.1.2 More covariates We can try the same analysis but with a more comprehensive set of covariates. library(boot) bootstrap_analysis &lt;- function(data, indices) { # Resample the data d &lt;- data[indices, ] # IPW # see: https://remlapmot.github.io/cibookex-r/ip-weighting-and-marginal-structural-models.html treat_mod &lt;- glm(qsmk ~ sex + race + age + I(age ^ 2) + as.factor(education) + smokeintensity + I(smokeintensity ^ 2) + smokeyrs + I(smokeyrs ^ 2) + as.factor(exercise) + as.factor(active) + wt71 + I(wt71 ^ 2), data = d, family = &quot;binomial&quot;) d$pX &lt;- predict(treat_mod, type = &quot;response&quot;) pn &lt;- glm(qsmk ~ 1, data = d, family = &quot;binomial&quot;) d$pnX &lt;- predict(pn, type = &quot;response&quot;) d$sw &lt;- with(d, ifelse(qsmk==1, pnX/pX, (1-pnX)/(1-pX))) # G-computation with IP weighted outcome model out_mod &lt;- lm(wt82_71 ~ qsmk + sex + race + age + I(age ^ 2) + as.factor(education) + smokeintensity + I(smokeintensity ^ 2) + smokeyrs + I(smokeyrs ^ 2) + as.factor(exercise) + as.factor(active) + wt71 + I(wt71 ^ 2), data = d, weights = sw) EX1 &lt;- predict(out_mod, newdata = transform(d, qsmk = 1)) EX0 &lt;- predict(out_mod, newdata = transform(d, qsmk = 0)) mean(EX1)-mean(EX0) # Return the coefficient of X return(mean(EX1)-mean(EX0)) } # Perform bootstrapping bootstrap_results &lt;- boot(data = d, statistic = bootstrap_analysis, R = n_bootstrap) # Summarize the bootstrap results bootstrap_summary &lt;- boot.ci(bootstrap_results, type = &quot;norm&quot;) # Print the results print(bootstrap_summary) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 100 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = bootstrap_results, type = &quot;norm&quot;) ## ## Intervals : ## Level Normal ## 95% ( 2.620, 4.393 ) ## Calculations and Intervals on Original Scale The overall inference is the same, although the more comprehensive adjustment set yields a slightly higher point estimate (around 3.5 kg), indicating that quitters gain even more weight than previously estimated. 4.2 Bootstrapped sub-group analysis bootstrap_analysis &lt;- function(data, indices) { # Resample the data d &lt;- data[indices, ] # IPW pn_sub &lt;- glm(qsmk ~ 1 + sex, data = d, family = &quot;binomial&quot;) d$pnX &lt;- predict(pn_sub, type = &quot;response&quot;) d$sw &lt;- with(d, ifelse(qsmk == 1, pnX / pX, (1 - pnX) / (1 - pX))) # G-computation with IP weighted outcome model out_mod &lt;- glm(wt82_71 ~ qsmk + sex + age + qsmk * sex, data = d, weights = sw) EX1S1 &lt;- predict(out_mod, newdata = transform(d, qsmk = 1, sex = as.factor(1))) EX1S0 &lt;- predict(out_mod, newdata = transform(d, qsmk = 1, sex = as.factor(0))) EX0S1 &lt;- predict(out_mod, newdata = transform(d, qsmk = 0, sex = as.factor(1))) EX0S0 &lt;- predict(out_mod, newdata = transform(d, qsmk = 0, sex = as.factor(0))) mean_diff_S1 &lt;- mean(EX1S1) - mean(EX0S1) mean_diff_S0 &lt;- mean(EX1S0) - mean(EX0S0) return(c(mean_diff_S1, mean_diff_S0)) } # Perform bootstrapping bootstrap_results &lt;- boot(data = d, statistic = bootstrap_analysis, R = n_bootstrap) # Extract and display results boot.ci(bootstrap_results, type = &quot;norm&quot;, index = 1) # For females ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 100 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = bootstrap_results, type = &quot;norm&quot;, index = 1) ## ## Intervals : ## Level Normal ## 95% ( 1.619, 4.043 ) ## Calculations and Intervals on Original Scale boot.ci(bootstrap_results, type = &quot;norm&quot;, index = 2) # For males ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 100 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = bootstrap_results, type = &quot;norm&quot;, index = 2) ## ## Intervals : ## Level Normal ## 95% ( 2.291, 4.585 ) ## Calculations and Intervals on Original Scale 4.3 Complex longitudinal exposure-outcome feedback In the book, we show a complicated DAG adapted from VanderWeele, Jackson, and Li (2016) of a complex longitudinal exposure-outcome feedback setting. Here, we verify that the adjustment strategy suggested in the book holds true in a simulated setting. To keep things simple, we set all effects to be recovered to 1. First, we simulate some data consistent with the complex DAG. # Seed for reproducibility set.seed(42) # Define sample size n &lt;- 1000 # Define variables C &lt;- rnorm(n) Z1 &lt;- rnorm(n, C) + rnorm(n) Z2 &lt;- rnorm(n, Z1) + rnorm(n) Z3 &lt;- rnorm(n, Z2) + rnorm(n) X1 &lt;- rnorm(n, C) + rnorm(n) X2 &lt;- rnorm(n, X1 + Z1) + rnorm(n) X3 &lt;- rnorm(n, X2 + Z2) + rnorm(n) Y &lt;- rnorm(n, X1 + X2 + X3 + Z3) + rnorm(n) Next, we fit a model for each measurement time point, and we see that all three models pick up the simulated effect of (roughly) 1. # Model for E[Y | X1, X2, Z1] to estimate effect of X1 on Y model_X1 &lt;- lm(Y ~ X1 + X2 + Z1) coef(model_X1)[[&quot;X1&quot;]] [1] 0.9999557 # Model for E[Y | X1, X2, X3, Z1] to estimate effect of X2 on Y model_X2 &lt;- lm(Y ~ X1 + X2 + X3 + Z1 + Z2) coef(model_X2)[[&quot;X2&quot;]] [1] 0.9896064 # Model for E[Y | X2, X3, Z2] to estimate effect of X3 on Y model_X3 &lt;- lm(Y ~ X2 + X3 + Z2) coef(model_X3)[[&quot;X3&quot;]] [1] 1.02467 4.4 Session info sessionInfo() ## R version 4.4.1 (2024-06-14 ucrt) ## Platform: x86_64-w64-mingw32/x64 ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=Danish_Denmark.utf8 LC_CTYPE=Danish_Denmark.utf8 ## [3] LC_MONETARY=Danish_Denmark.utf8 LC_NUMERIC=C ## [5] LC_TIME=Danish_Denmark.utf8 ## ## time zone: Europe/Copenhagen ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] cobalt_4.5.5 causaldata_0.1.4 marginaleffects_0.23.0 ## [4] dplyr_1.1.4 tidybayes_3.0.6 brms_2.21.0 ## [7] Rcpp_1.0.12 sandwich_3.1-1 boot_1.3-30 ## [10] patchwork_1.3.0 ggplot2_3.5.1 ## ## loaded via a namespace (and not attached): ## [1] gtable_0.3.5 tensorA_0.36.2.1 xfun_0.48 ## [4] bslib_0.7.0 QuickJSR_1.2.2 insight_0.20.5 ## [7] collapse_2.0.16 inline_0.3.19 lattice_0.22-6 ## [10] vctrs_0.6.5 tools_4.4.1 generics_0.1.4 ## [13] stats4_4.4.1 parallel_4.4.1 tibble_3.2.1 ## [16] highr_0.11 pkgconfig_2.0.3 Matrix_1.7-0 ## [19] data.table_1.15.4 checkmate_2.3.1 distributional_0.5.0 ## [22] RcppParallel_5.1.7 lifecycle_1.0.4 compiler_4.4.1 ## [25] farver_2.1.2 stringr_1.5.1 Brobdingnag_1.2-9 ## [28] munsell_0.5.1 codetools_0.2-20 htmltools_0.5.8.1 ## [31] sass_0.4.9 bayesplot_1.11.1 yaml_2.3.8 ## [34] crayon_1.5.3 tidyr_1.3.1 pillar_1.10.2 ## [37] jquerylib_0.1.4 arrayhelpers_1.1-0 cachem_1.1.0 ## [40] StanHeaders_2.35.0.9000 bridgesampling_1.1-2 abind_1.4-8 ## [43] nlme_3.1-164 posterior_1.6.1 rstan_2.35.0.9000 ## [46] svUnit_1.0.6 tidyselect_1.2.1 digest_0.6.35 ## [49] mvtnorm_1.2-5 stringi_1.8.4 purrr_1.0.2 ## [52] bookdown_0.41 labeling_0.4.3 splines_4.4.1 ## [55] fastmap_1.2.0 grid_4.4.1 colorspace_2.1-0 ## [58] cli_3.6.2 magrittr_2.0.3 loo_2.8.0 ## [61] pkgbuild_1.4.4 withr_3.0.2 scales_1.3.0 ## [64] backports_1.5.0 estimability_1.5.1 rmarkdown_2.27 ## [67] matrixStats_1.3.0 emmeans_1.10.5 gridExtra_2.3 ## [70] chk_0.9.2 zoo_1.8-12 coda_0.19-4.1 ## [73] evaluate_1.0.4 knitr_1.47 ggdist_3.3.2 ## [76] mgcv_1.9-1 rstantools_2.4.0 rlang_1.1.4 ## [79] xtable_1.8-4 glue_1.7.0 rstudioapi_0.16.0 ## [82] jsonlite_1.8.8 R6_2.5.1 References "],["chapter-5.html", "Chapter 5 Most of Your Data is Almost Always Missing 5.1 Simulating missingness 5.2 Poststratification 5.3 Instrumental variable analysis 5.4 Bayesian instrumental variable analysis 5.5 Session info", " Chapter 5 Most of Your Data is Almost Always Missing 5.1 Simulating missingness We first simulate informative missingness where the outcome is associated with sampling… set.seed(2025) n &lt;- 1e3 bX &lt;- 2 X &lt;- rnorm(n, 0, 1) Y &lt;- bX*X + rnorm(n, 10, 5) prob_missing &lt;- plogis(Y - mean(Y)) miss &lt;- runif(n) &lt; prob_missing Y_miss &lt;- Y Y_miss[miss] &lt;- NA d_miss &lt;- data.frame(X = X, Y = Y, Y_miss = Y_miss) … And then fit two different models: First, a model fitted only on the non-missing observations and then a model on all the data, as if we had access to the full population. lm(Y_miss ~ X, data = d_miss) lm(Y ~ X, data = d_miss) library(ggplot2) ggplot(d_miss, aes(x = X, y = Y)) + geom_point(aes(color = is.na(Y_miss)), alpha = 0.5) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, linetype = &quot;solid&quot;, color = &quot;black&quot;, data = d_miss[!is.na(Y_miss), ]) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, linetype = &quot;solid&quot;, color = &quot;grey60&quot;, data = d_miss) + scale_color_manual(values = c(&quot;black&quot;, &quot;grey&quot;), labels = c(&quot;Observed&quot;, &quot;Missing&quot;)) + labs(title = &quot;Simulating missingness&quot;, subtitle = &quot;Models with (black) and without (grey) missingness&quot;, x = &quot;X&quot;, y = &quot;Y&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) 5.2 Poststratification To illustrate poststratification, we return to the nhefs dataset. We also load the US 2021 census, which will help us re-weight out model predictions to the greater US population. library(causaldata) d &lt;- nhefs d$sex &lt;- as.factor(d$sex) census2021 &lt;- read.csv(&quot;data/CensusUS2021.csv&quot;) We then re-score the census and the nhefs data, such that the variable levels are consistent. We also calculate census proportions from the percentages in the original census data set. census &lt;- census2021 census$age_group[census$AGE_GROUP == &quot;Under 15 years&quot;] &lt;- 1 census$age_group[census$AGE_GROUP == &quot;15 to 17 years&quot;] &lt;- 2 census$age_group[census$AGE_GROUP == &quot;18 to 20 years&quot;] &lt;- 3 census$age_group[census$AGE_GROUP == &quot;21 to 44 years&quot;] &lt;- 4 census$age_group[census$AGE_GROUP == &quot;45 to 64 years&quot;] &lt;- 5 census$age_group[census$AGE_GROUP == &quot;65 years and over&quot;] &lt;- 6 census$sex &lt;- ifelse(census$SEX == &quot;FEMALE&quot;, 1, 0) |&gt; as.factor() census$proportion &lt;- census$PERCENTAGE/100 write.csv(census, &quot;data/census_ageGroups.csv&quot;, row.names = FALSE) d2 &lt;- d d2$age_group[d2$age &lt; 15] &lt;- 1 # &quot;Under 15 years&quot; d2$age_group[d2$age &gt;= 15 &amp; d2$age &lt;= 17] &lt;- 2 # &quot;15 to 17 years&quot; d2$age_group[d2$age &gt;= 18 &amp; d2$age &lt;= 20] &lt;- 3 # &quot;18 to 20 years&quot; d2$age_group[d2$age &gt;= 21 &amp; d2$age &lt;= 44] &lt;- 4 # &quot;21 to 44 years&quot; d2$age_group[d2$age &gt;= 45 &amp; d2$age &lt;= 64] &lt;- 5 # &quot;45 to 64 years&quot; d2$age_group[d2$age &gt; 64] &lt;- 6 # &quot;65 years and over&quot; write.csv(d2, &quot;data/nhefs_ageGroups.csv&quot;, row.names = FALSE) We can then check how the two datasets compare in their distributions of the covariates age and sex. library(patchwork) library(dplyr) library(ggplot2) ### Calculate demographic proportions in the nhefs data... d3 &lt;- d2 |&gt; group_by(age_group, sex) |&gt; summarise(n = n()) |&gt; mutate(proportion = n/sum(n)) |&gt; ungroup() ### ... And fill in missing demographic combinations d3 &lt;- rbind(data.frame(age_group = c(1,1,2,2,3,3), sex = rep(c(0,1), 3), n = 0, proportion = 0), d3) ### Make age_group a factor variable d3$age_group &lt;- factor(d3$age_group, levels = 1:6) census$age_group &lt;- factor(census$age_group, levels = 1:6) ### Plotting distributions of age groups for each sex in nhefs and census ## Males p1 &lt;- ggplot() + geom_line(data = subset(d3, sex == 0), aes(x = age_group, y = proportion, group = 1), linetype = &quot;dashed&quot;, linewidth = 1) + geom_line(data = subset(census, SEX == &quot;MALE&quot;), aes(x = age_group, y = PERCENTAGE/100, group = 1), linewidth = 1) + theme_classic() + theme( axis.text.y = element_blank(), axis.ticks.y = element_blank(), axis.title.y = element_blank(), axis.line.y = element_blank(), axis.text.x = element_text(angle = -60, vjust = -1)) + scale_x_discrete(NULL, labels = c( &quot;1&quot; = &quot;Under 15 years&quot;, &quot;2&quot; = &quot;15 to 17 years&quot;, &quot;3&quot; = &quot;18 to 20 years&quot;, &quot;4&quot; = &quot;21 to 44 years&quot;, &quot;5&quot; = &quot;45 to 64 years&quot;, &quot;6&quot; = &quot;+65 years&quot;)) + labs(title = &quot;Age distributions&quot;, subtitle = &quot;Males&quot;) ## Females p2 &lt;- ggplot() + geom_line(data = subset(d3, sex == 1), aes(x = age_group, y = proportion, group = 1), linetype = &quot;dashed&quot;, linewidth = 1) + geom_line(data = subset(census, SEX == &quot;FEMALE&quot;), aes(x = age_group, y = PERCENTAGE/100, group = 1), linewidth = 1) + theme_classic() + theme( axis.text.y = element_blank(), axis.ticks.y = element_blank(), axis.title.y = element_blank(), axis.line.y = element_blank(), axis.text.x = element_text(angle = -60, vjust = -1)) + scale_x_discrete(NULL, labels = c( &quot;1&quot; = &quot;Under 15 years&quot;, &quot;2&quot; = &quot;15 to 17 years&quot;, &quot;3&quot; = &quot;18 to 20 years&quot;, &quot;4&quot; = &quot;21 to 44 years&quot;, &quot;5&quot; = &quot;45 to 64 years&quot;, &quot;6&quot; = &quot;+65 years&quot;)) + labs(subtitle = &quot;Females&quot;) (p1 + p2) 5.2.1 Frequentist poststratification We show two different poststratification implementations. First, a basic frequentist approach – see e.g. also https://github.com/RohanAlexander/mrp_workshop/blob/master/getting-started-with-mrp.Rmd First, we fit a model on the re-scored nhefs data (stored in d2) and include interactions between the exposure qsmk and each of the two covariates (sex and age&gt;_group) to make the model reasonably flexible. mod &lt;- lm(wt82_71 ~ qsmk*sex + qsmk*age_group, data = d2) We then apply g-computation with a twist. The twist is that we use the model fitted on nhefs data to get potential outcomes predictions (\\(Y^{X=1}\\) and \\(Y^{X=0}\\)) for the census data frame and then weight our predictions with the demographic proportions of each combination of sex and age group in the greater US population according to the census. We finally calculate the difference in means between the weighted predictions to get a marginal effect estimate, which we find to be around 7 kg. Before we can do any of it, however, we need to make sure that age_group is treated as numeric and sex as a factor, since this was the variable types used for model fitting. census$age_group &lt;- as.numeric(census$age_group) census$sex &lt;- as.factor(census$sex) census$EX1 &lt;- predict(mod, newdata = transform(census, qsmk = 1)) census$EX0 &lt;- predict(mod, newdata = transform(census, qsmk = 0)) census$wEX1 &lt;- census$EX1*census$proportion census$wEX0 &lt;- census$EX0*census$proportion with(census, sum(wEX1)-sum(wEX0)) ## [1] 6.991724 Here’s the code to reproduce the accompanying table in the text. xtable::xtable(census[c(&quot;AGE_GROUP&quot;, &quot;SEX&quot;, &quot;EX1&quot;, &quot;EX0&quot;,&quot;wEX1&quot;, &quot;wEX0&quot;)], digits = c(0,0,0,1,1,1,1), include.rownames=FALSE) ## % latex table generated in R 4.4.1 by xtable 1.8-4 package ## % Sun Aug 24 22:07:42 2025 ## \\begin{table}[ht] ## \\centering ## \\begin{tabular}{rllrrrr} ## \\hline ## &amp; AGE\\_GROUP &amp; SEX &amp; EX1 &amp; EX0 &amp; wEX1 &amp; wEX0 \\\\ ## \\hline ## 1 &amp; Under 15 years &amp; MALE &amp; 19.4 &amp; 13.1 &amp; 3.7 &amp; 2.5 \\\\ ## 2 &amp; 15 to 17 years &amp; MALE &amp; 15.4 &amp; 10.0 &amp; 0.6 &amp; 0.4 \\\\ ## 3 &amp; 18 to 20 years &amp; MALE &amp; 11.5 &amp; 6.8 &amp; 0.4 &amp; 0.3 \\\\ ## 4 &amp; 21 to 44 years &amp; MALE &amp; 7.5 &amp; 3.7 &amp; 2.4 &amp; 1.2 \\\\ ## 5 &amp; 45 to 64 years &amp; MALE &amp; 3.6 &amp; 0.5 &amp; 0.9 &amp; 0.1 \\\\ ## 6 &amp; 65 years and over &amp; MALE &amp; -0.4 &amp; -2.6 &amp; -0.1 &amp; -0.4 \\\\ ## 7 &amp; Under 15 years &amp; FEMALE &amp; 18.3 &amp; 12.9 &amp; 3.2 &amp; 2.3 \\\\ ## 8 &amp; 15 to 17 years &amp; FEMALE &amp; 14.4 &amp; 9.7 &amp; 0.5 &amp; 0.4 \\\\ ## 9 &amp; 18 to 20 years &amp; FEMALE &amp; 10.4 &amp; 6.6 &amp; 0.4 &amp; 0.2 \\\\ ## 10 &amp; 21 to 44 years &amp; FEMALE &amp; 6.5 &amp; 3.4 &amp; 2.0 &amp; 1.1 \\\\ ## 11 &amp; 45 to 64 years &amp; FEMALE &amp; 2.5 &amp; 0.3 &amp; 0.6 &amp; 0.1 \\\\ ## 12 &amp; 65 years and over &amp; FEMALE &amp; -1.5 &amp; -2.9 &amp; -0.3 &amp; -0.5 \\\\ ## \\hline ## \\end{tabular} ## \\end{table} 5.2.2 Bayesian poststratification We finally show a fully Bayesian poststratification routine. To showcase the difference between a single-level (i.e., fixed effects) and a multilevel (i.e., random effects) model for poststratification, we fit a Bayesian multilevel model and contrast that with the frequentist model predictions stored in census. For fitting, we again use the brms package with default priors and a seed for reproducibility. We allow the effect of the exposure qsmk vary within both sex and age_group using common R formula syntax. library(brms) bayes_mod &lt;- brm(wt82_71 ~ 1 + (1 + qsmk | sex) + (1 + qsmk | age_group), data = d2, cores = 4, seed = 42, file = &quot;fits/bayes_poststrat&quot;) Next, we do two things with this model. First, we compute the posterior mean predictions for each covariate combination without applying poststratification. library(tidybayes) ### Predict outcome when exposure X = 1 for all psEX1 &lt;- add_epred_draws(object = bayes_mod, newdata = transform(census, qsmk=1), # Predict for covariates combinations # not observed in the training data (nhefs) allow_new_levels = TRUE) |&gt; # Posterior means for each covariate combination group_by(age_group, sex) |&gt; summarise(psEX1 = mean(.epred)) ### Predict outcome when exposure X = 0 for all psEX0 &lt;- add_epred_draws(object = bayes_mod, newdata = transform(census, qsmk=0), allow_new_levels = TRUE) |&gt; group_by(age_group, sex) |&gt; summarise(psEX0 = mean(.epred)) Then, we again compute the marginal mean predictions for each covariate combination but this time applying poststratification weights. Everything else is the same. ### Predict outcome when exposure X = 1 for all wpsEX1 &lt;- add_epred_draws(object = bayes_mod, newdata = transform(census, qsmk=1), allow_new_levels = TRUE) |&gt; # New bit: Re-weight model predictions using census proportions mutate(estimate_prop = .epred*proportion) |&gt; group_by(age_group, sex, .draw) |&gt; summarise(wpsEX1 = sum(estimate_prop)) |&gt; # Posterior means for each covariate combination group_by(age_group, sex) |&gt; summarise(wpsEX1 = mean(wpsEX1)) ### Predict outcome when exposure X = 0 for all wpsEX0 &lt;- add_epred_draws(object = bayes_mod, newdata = transform(census, qsmk=0), allow_new_levels = TRUE) |&gt; mutate(estimate_prop = .epred*proportion) |&gt; group_by(age_group, sex, .draw) |&gt; summarise(wpsEX0 = sum(estimate_prop)) |&gt; group_by(age_group, sex) |&gt; summarise(wpsEX0 = mean(wpsEX0)) We then collect the posterior mean predictions and the frequentist predictions in two data frames, without and with poststratification… ### No poststratification ps &lt;- data.frame(age_group = census$AGE_GROUP, sex = census$SEX, EX1 = census$EX1, EX0 = census$EX0, psEX1 = psEX1$psEX1, psEX0 = psEX0$psEX0, E = census$EX1 - census$EX0, psE = psEX1$psEX1 - psEX0$psEX0) ps$age_group &lt;- factor(ps$age_group, levels = c( &quot;1&quot; = &quot;Under 15 years&quot;, &quot;2&quot; = &quot;15 to 17 years&quot;, &quot;3&quot; = &quot;18 to 20 years&quot;, &quot;4&quot; = &quot;21 to 44 years&quot;, &quot;5&quot; = &quot;45 to 64 years&quot;, &quot;6&quot; = &quot;65 years and over&quot;)) ### With poststratification psw &lt;- data.frame(age_group = census$AGE_GROUP, sex = census$SEX, wEX1 = census$wEX1, wEX0 = census$wEX0, wpsEX1 = wpsEX1$wpsEX1, wpsEX0 = wpsEX0$wpsEX0) psw$age_group &lt;- factor(psw$age_group, levels = c( &quot;1&quot; = &quot;Under 15 years&quot;, &quot;2&quot; = &quot;15 to 17 years&quot;, &quot;3&quot; = &quot;18 to 20 years&quot;, &quot;4&quot; = &quot;21 to 44 years&quot;, &quot;5&quot; = &quot;45 to 64 years&quot;, &quot;6&quot; = &quot;65 years and over&quot;)) … And plot the comparisons. p1 &lt;- ggplot() + geom_line(data = subset(ps, sex == &quot;FEMALE&quot;), aes(x = age_group, y = EX1, group = 1), linetype = &quot;dashed&quot;, size = 1, colour = &quot;grey60&quot;) + geom_line(data = subset(ps, sex == &quot;FEMALE&quot;), aes(x = age_group, y = psEX1, group = 1), size = 1, colour = &quot;grey60&quot;) + geom_line(data = subset(ps, sex == &quot;FEMALE&quot;), aes(x = age_group, y = EX0, group = 1), linetype = &quot;dashed&quot;, size = 1) + geom_line(data = subset(ps, sex == &quot;FEMALE&quot;), aes(x = age_group, y = psEX0, group = 1), size = 1) + theme_classic() + ylab(&quot;Weight change (kg)&quot;) + theme( axis.text.x = element_text(angle = -60, vjust = -1)) + scale_x_discrete(NULL, labels = c( &quot;1&quot; = &quot;Under 15 years&quot;, &quot;2&quot; = &quot;15 to 17 years&quot;, &quot;3&quot; = &quot;18 to 20 years&quot;, &quot;4&quot; = &quot;21 to 44 years&quot;, &quot;5&quot; = &quot;45 to 64 years&quot;, &quot;65 years and over&quot; = &quot;+65 years&quot;)) + labs(title = &quot;Predicted weight change&quot;, subtitle = &quot;No poststratification&quot;) p2 &lt;- ggplot() + geom_line(data = subset(psw, sex == &quot;FEMALE&quot;), aes(x = age_group, y = wEX1, group = 1), linetype = &quot;dashed&quot;, size = 1, colour = &quot;grey60&quot;) + geom_line(data = subset(psw, sex == &quot;FEMALE&quot;), aes(x = age_group, y = wpsEX1, group = 1), size = 1, colour = &quot;grey60&quot;) + geom_line(data = subset(psw, sex == &quot;FEMALE&quot;), aes(x = age_group, y = wEX0, group = 1), linetype = &quot;dashed&quot;, size = 1) + geom_line(data = subset(psw, sex == &quot;FEMALE&quot;), aes(x = age_group, y = wpsEX0, group = 1), size = 1) + theme_classic() + ylab(NULL) + theme( axis.text.x = element_text(angle = -60, vjust = -1)) + scale_x_discrete(NULL, labels = c( &quot;1&quot; = &quot;Under 15 years&quot;, &quot;2&quot; = &quot;15 to 17 years&quot;, &quot;3&quot; = &quot;18 to 20 years&quot;, &quot;4&quot; = &quot;21 to 44 years&quot;, &quot;5&quot; = &quot;45 to 64 years&quot;, &quot;65 years and over&quot; = &quot;+65 years&quot;)) + labs(subtitle = &quot;With poststratification&quot;) (p1 + p2) In the text, we showed predictions for females only, but the results are similar for males. p3 &lt;- ggplot() + geom_line(data = subset(ps, sex == &quot;MALE&quot;), aes(x = age_group, y = EX1, group = 1), linetype = &quot;dashed&quot;, size = 1, colour = &quot;grey60&quot;) + geom_line(data = subset(ps, sex == &quot;MALE&quot;), aes(x = age_group, y = psEX1, group = 1), size = 1, colour = &quot;grey60&quot;) + geom_line(data = subset(ps, sex == &quot;MALE&quot;), aes(x = age_group, y = EX0, group = 1), linetype = &quot;dashed&quot;, size = 1) + geom_line(data = subset(ps, sex == &quot;MALE&quot;), aes(x = age_group, y = psEX0, group = 1), size = 1) + theme_classic() + ylab(&quot;Weight change (kg)&quot;) + theme( axis.text.x = element_text(angle = -60, vjust = -1)) + scale_x_discrete(NULL, labels = c( &quot;1&quot; = &quot;Under 15 years&quot;, &quot;2&quot; = &quot;15 to 17 years&quot;, &quot;3&quot; = &quot;18 to 20 years&quot;, &quot;4&quot; = &quot;21 to 44 years&quot;, &quot;5&quot; = &quot;45 to 64 years&quot;, &quot;65 years and over&quot; = &quot;+65 years&quot;)) + labs(title = &quot;Predicted weight change&quot;, subtitle = &quot;No poststratification (males)&quot;) p4 &lt;- ggplot() + geom_line(data = subset(psw, sex == &quot;MALE&quot;), aes(x = age_group, y = wEX1, group = 1), linetype = &quot;dashed&quot;, size = 1, colour = &quot;grey60&quot;) + geom_line(data = subset(psw, sex == &quot;MALE&quot;), aes(x = age_group, y = wpsEX1, group = 1), size = 1, colour = &quot;grey60&quot;) + geom_line(data = subset(psw, sex == &quot;MALE&quot;), aes(x = age_group, y = wEX0, group = 1), linetype = &quot;dashed&quot;, size = 1) + geom_line(data = subset(psw, sex == &quot;MALE&quot;), aes(x = age_group, y = wpsEX0, group = 1), size = 1) + theme_classic() + ylab(NULL) + theme( axis.text.x = element_text(angle = -60, vjust = -1)) + scale_x_discrete(NULL, labels = c( &quot;1&quot; = &quot;Under 15 years&quot;, &quot;2&quot; = &quot;15 to 17 years&quot;, &quot;3&quot; = &quot;18 to 20 years&quot;, &quot;4&quot; = &quot;21 to 44 years&quot;, &quot;5&quot; = &quot;45 to 64 years&quot;, &quot;65 years and over&quot; = &quot;+65 years&quot;)) + labs(subtitle = &quot;With poststratification (males)&quot;) (p3 + p4) 5.2.2.1 Poststratified marginal causal effect The above workflow computed and plotted posterior means, but with a Bayesian model we have a full posterior distribution to work with. So let’s do that. This time, we’re interested in the poststratified estimate for the population as a whole. This means that we have to marginalize over the covariates, which we do by grouping on posterior draws instead of on the covariates. The rest should look familiar. ### Predict outcome when exposure X = 1 for all ateEX1 &lt;- add_epred_draws(object = bayes_mod, newdata = transform(census, qsmk=1), allow_new_levels = TRUE) |&gt; mutate(estimate_prop = .epred*proportion) |&gt; # New bit: Marginalize over covariate combinations by grouping on .draw group_by(.draw) |&gt; summarise(.epred = sum(estimate_prop)) ### Predict outcome when exposure X = 1 for all ateEX0 &lt;- add_epred_draws(object = bayes_mod, newdata = transform(census, qsmk=0), allow_new_levels = TRUE) |&gt; mutate(estimate_prop = .epred*proportion) |&gt; group_by(.draw) |&gt; summarise(.epred = sum(estimate_prop)) ### Compute poststratified ATE poststratified_ate &lt;- data.frame(EX1 = ateEX1$.epred, EX0 = ateEX0$.epred, draw = ateEX0$.draw) |&gt; # For each posterior draw... group_by(draw) |&gt; # ... Calculate ATE summarise(ate = mean(EX1 - EX0)) The poststratified marginal causal effect is around 4.7 kg but with a fairly wide 95% interval ranging from around 0 to 9 kg. mean_hdi(poststratified_ate$ate) ## y ymin ymax .width .point .interval ## 1 4.677918 0.1554798 8.870636 0.95 mean hdi Behold the full posterior poststratified marginal causal effect! ggplot(poststratified_ate, aes(x = ate)) + geom_density() + theme_classic() 5.3 Instrumental variable analysis 5.3.1 Preparing Cohen et al. (2015) We’ll use the data ACT_IllLvlMainWithMalProbs_FINAL_pub.dta from Cohen, Dupas, and Schaner (2015), which can be downloaded from the book’s Github page or www.openicpsr.org. The data need to be wrangled a little before use. Each row is an illness period, where most households only have a single illness period. A few households, however, do have more. The models we use here assume – as did the original study – illness periods to be independent. library(haven) # for loading .dta file library(dplyr) ### Load original data dta &lt;- read_dta(&quot;data/ACT_IllLvlMainWithMalProbs_FINAL_pub.dta&quot;) |&gt; as.data.frame() ### Filter data set, as the original study did -- re-use name from original analysis script all_ill_prob &lt;- subset(dta, first_ep==1 &amp; ex_post==0 &amp; rdt_any==0) ### Collaps all ACT subsidy types all_ill_prob$act_any &lt;- ifelse(all_ill_prob$act40==1 | all_ill_prob$act60==1 | all_ill_prob$act100==1, 1, 0) |&gt; as.integer() ### Replace sample-mean imputed values with NAs to allow Bayesian imputation, and then standardize all_ill_prob$B_head_age_bimps &lt;- with(all_ill_prob, ifelse(B_head_age_missing==1, NA, B_head_age_imputed)) ### Prepare data for IV and multilevel analysis cohen2015 &lt;- all_ill_prob |&gt; select(householdid, took_act, act_any, B_head_age_bimps, head_lit, used_act_v, totstrata) ### Change col names colnames(cohen2015) &lt;- c(&quot;hid&quot;, &quot;act&quot;, &quot;subsidy&quot;, &quot;age&quot;, &quot;literate&quot;, &quot;voucher&quot;, &quot;stratum&quot;) ### Declare variable types cohen2015$hid &lt;- as.factor(cohen2015$hid) cohen2015$act &lt;- as.integer(cohen2015$act) cohen2015$subsidy &lt;- as.integer(cohen2015$subsidy) cohen2015$age &lt;- as.numeric(cohen2015$age) cohen2015$literate &lt;- as.integer(cohen2015$literate) cohen2015$voucher &lt;- as.integer(cohen2015$voucher) cohen2015$stratum &lt;- as.factor(cohen2015$stratum) ### Subset data for missing data analysis cohen2015miss &lt;- cohen2015 |&gt; select(act, subsidy, age, literate) ### Export write.csv(cohen2015, &quot;data/cohen2015.csv&quot;, row.names = FALSE) write.csv(cohen2015miss, &quot;data/cohen2015miss.csv&quot;, row.names = FALSE) 5.3.2 Randomized treatment assignment as instrument d &lt;- read.csv(&quot;data/cohen2015.csv&quot;) ### Fit outcome and treatment models y_mod &lt;- glm(act ~ subsidy, data = d, family = &quot;binomial&quot;) x_mod &lt;- glm(voucher ~ subsidy, data = d, family = &quot;binomial&quot;) ### Intention-to-treat analysis YV1 &lt;- predict(y_mod, newdata = transform(d, subsidy = 1), type = &quot;response&quot;) YV0 &lt;- predict(y_mod, newdata = transform(d, subsidy = 0), type = &quot;response&quot;) itt &lt;- mean(YV1) - mean(YV0) ### &quot;Compliance analysis&quot; XV1 &lt;- predict(x_mod, newdata = transform(d, subsidy = 1), type = &quot;response&quot;) XV0 &lt;- predict(x_mod, newdata = transform(d, subsidy = 0), type = &quot;response&quot;) compliance &lt;- mean(XV1) - mean(XV0) ### Wald IV ratio estimator / Treatment-on-the-treated estimate (iv &lt;- itt/compliance) ## [1] 0.9551056 5.4 Bayesian instrumental variable analysis As mentioned in the text, we also want to demonstrate another approach to instrumental variable analysis, namely a Bayesian implementation. Kurz (2023, ch. 14) gives a rundown of instrumental variable analysis using the brms package, building on McElreath (2020), and we follow that general approach here. We refer to those sources for further details. We first define two model formulas, one for the instrument (subsidy) predicting treatment (voucher) \\(\\textrm{E}[X \\mid V]\\) and another for the treatment predicting ACT uptake (act) model \\(\\textrm{E}[Y \\mid X]\\). This is superficially similar to how we fitted y_mod and x_mod in the frequentist setting in the text and above but note a few differences: First, while the model predicting voucher use from subsidy assignment corresponds to the “compliance analysis” (x_mod), the model predicting ACT uptake from voucher use is not used for the Wald estimator. Second, in the Bayesian setup we fit the two models in the same go. In brms this is facilitated by wrapping the model formulas in bf() and then combining these in the model fitting call using +. Finally, we allow the model to estimate the residual correlation between these two models by setting set_rescor(TRUE). Why does this work as an alternative IV estimator? Recall that we use an instrumental variable approach to deal with situations where the treatment variable (here, subsidy) is correlated with the error term of the outcome model – in essence, there’s an open backdoor path between treatment and the outcome. This residual correlation could be due to unobserved confounding variables of some sort, which in turn will lead to biased estimates in a simple regression model. We account for the possibility that such confounding exists by explicitly modeling this correlation. library(brms) # Treatment-instrument model formula xv_formula &lt;- bf(voucher ~ subsidy) # Outcome-treatment model formula yx_formula &lt;- bf(act ~ voucher) # Fit models and set residual correlation = TRUE bayes_iv_mod &lt;- brm(xv_formula + yx_formula + set_rescor(TRUE), data = d, cores = 4, seed = 42, file = &quot;fits/bayes_iv&quot;) By inspecting the summary, we see that we get essentially the same results as above – act_voucher corresponds to the Wald esimate – except we now have a posterior distribution to work with. summary(bayes_iv_mod) ## Family: MV(gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: voucher ~ subsidy ## act ~ voucher ## Data: d (Number of observations: 631) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Regression Coefficients: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## voucher_Intercept 0.13 0.03 0.06 0.20 1.00 2810 2312 ## act_Intercept 0.07 0.06 -0.05 0.18 1.00 1313 1228 ## voucher_subsidy 0.20 0.04 0.13 0.28 1.00 2455 2216 ## act_voucher 0.96 0.20 0.60 1.40 1.00 1294 1226 ## ## Further Distributional Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_voucher 0.44 0.01 0.42 0.46 1.00 3324 2357 ## sigma_act 0.44 0.04 0.39 0.54 1.00 1366 1265 ## ## Residual Correlations: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## rescor(voucher,act) -0.38 0.17 -0.68 -0.02 1.00 1312 1184 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now, since we don’t include covariates in these models, it’s safe to just work with the coefficients here; the marginal and conditional estimates are the same in this particular case. However, in cases where the conditional and marginal effects differ, what we’ve done here is to calculate a conditional estimate, since we’re simply working with the coefficients. To get a marginal estimate in such a case, we’d have to implement something like g-computation. An explicitly marginal workflow could look like the following, following a familiar g-computation approach: # Calculate predicted values for voucher = 1 and voucher = 0 bayes_YX1 &lt;- add_epred_draws(object = bayes_iv_mod, newdata = transform(d, voucher = 1), resp = &quot;act&quot;) bayes_YX0 &lt;- add_epred_draws(object = bayes_iv_mod, newdata = transform(d, voucher = 0), resp = &quot;act&quot;) # Bayesian marginal IV bayes_marginal_iv &lt;- data.frame(EX1 = bayes_YX1$.epred, EX0 = bayes_YX0$.epred, draw = bayes_YX0$.draw) |&gt; # For each posterior draw... group_by(draw) |&gt; # ... Calculate ATE summarise(late = mean(EX1 - EX0)) Again, in this simple example, the conditional and marginal IV results are identical. mean_qi(bayes_marginal_iv$late) ## y ymin ymax .width .point .interval ## 1 0.9575332 0.6015686 1.397755 0.95 mean qi 5.5 Session info sessionInfo() ## R version 4.4.1 (2024-06-14 ucrt) ## Platform: x86_64-w64-mingw32/x64 ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=Danish_Denmark.utf8 LC_CTYPE=Danish_Denmark.utf8 ## [3] LC_MONETARY=Danish_Denmark.utf8 LC_NUMERIC=C ## [5] LC_TIME=Danish_Denmark.utf8 ## ## time zone: Europe/Copenhagen ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] haven_2.5.4 cobalt_4.5.5 causaldata_0.1.4 ## [4] marginaleffects_0.23.0 dplyr_1.1.4 tidybayes_3.0.6 ## [7] brms_2.21.0 Rcpp_1.0.12 sandwich_3.1-1 ## [10] boot_1.3-30 patchwork_1.3.0 ggplot2_3.5.1 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_1.2.1 svUnit_1.0.6 farver_2.1.2 ## [4] loo_2.8.0 fastmap_1.2.0 tensorA_0.36.2.1 ## [7] digest_0.6.35 estimability_1.5.1 lifecycle_1.0.4 ## [10] StanHeaders_2.35.0.9000 magrittr_2.0.3 posterior_1.6.1 ## [13] compiler_4.4.1 rlang_1.1.4 sass_0.4.9 ## [16] tools_4.4.1 yaml_2.3.8 data.table_1.15.4 ## [19] collapse_2.0.16 knitr_1.47 labeling_0.4.3 ## [22] bridgesampling_1.1-2 pkgbuild_1.4.4 plyr_1.8.9 ## [25] abind_1.4-8 withr_3.0.2 purrr_1.0.2 ## [28] grid_4.4.1 stats4_4.4.1 xtable_1.8-4 ## [31] colorspace_2.1-0 inline_0.3.19 emmeans_1.10.5 ## [34] scales_1.3.0 insight_0.20.5 cli_3.6.2 ## [37] mvtnorm_1.2-5 rmarkdown_2.27 crayon_1.5.3 ## [40] generics_0.1.4 RcppParallel_5.1.7 rstudioapi_0.16.0 ## [43] reshape2_1.4.4 tzdb_0.4.0 cachem_1.1.0 ## [46] rstan_2.35.0.9000 stringr_1.5.1 splines_4.4.1 ## [49] bayesplot_1.11.1 parallel_4.4.1 matrixStats_1.3.0 ## [52] vctrs_0.6.5 Matrix_1.7-0 jsonlite_1.8.8 ## [55] bookdown_0.41 hms_1.1.3 arrayhelpers_1.1-0 ## [58] ggdist_3.3.2 jquerylib_0.1.4 tidyr_1.3.1 ## [61] glue_1.7.0 chk_0.9.2 codetools_0.2-20 ## [64] distributional_0.5.0 stringi_1.8.4 gtable_0.3.5 ## [67] QuickJSR_1.2.2 munsell_0.5.1 tibble_3.2.1 ## [70] pillar_1.10.2 htmltools_0.5.8.1 Brobdingnag_1.2-9 ## [73] R6_2.5.1 evaluate_1.0.4 lattice_0.22-6 ## [76] readr_2.1.5 highr_0.11 backports_1.5.0 ## [79] bslib_0.7.0 rstantools_2.4.0 coda_0.19-4.1 ## [82] gridExtra_2.3 nlme_3.1-164 checkmate_2.3.1 ## [85] mgcv_1.9-1 xfun_0.48 zoo_1.8-12 ## [88] forcats_1.0.0 pkgconfig_2.0.3 References "],["chapter-6.html", "Chapter 6 More Missing Data 6.1 Simple mean imputation vs. multiple imputation 6.2 “Combine then predict” or “predict then combine”? 6.3 Bayesian imputation 6.4 Session info", " Chapter 6 More Missing Data 6.1 Simple mean imputation vs. multiple imputation Here we illustrate simple mean imputation. d &lt;- read.csv(&quot;data/cohen2015miss.csv&quot;) d$age &lt;- scale(d$age) |&gt; as.numeric() d_mimp &lt;- d d_mimp$age &lt;- with(d_mimp, ifelse(is.na(age), mean(age, na.rm = T), age)) lm(act ~ subsidy + age, data = d_mimp) |&gt; summary() ## ## Call: ## lm(formula = act ~ subsidy + age, data = d_mimp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.4774 -0.3866 -0.2396 0.5566 0.9431 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.19126 0.03555 5.380 1.05e-07 *** ## subsidy 0.19535 0.04151 4.706 3.11e-06 *** ## age -0.06592 0.01892 -3.485 0.000527 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4608 on 628 degrees of freedom ## (34 observations deleted due to missingness) ## Multiple R-squared: 0.05275, Adjusted R-squared: 0.04973 ## F-statistic: 17.49 on 2 and 628 DF, p-value: 4.074e-08 We can compare with a complete-case analysis… lm(act ~ subsidy + age, data = d) |&gt; summary() ## ## Call: ## lm(formula = act ~ subsidy + age, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.4908 -0.3978 -0.2439 0.5472 0.9378 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.19579 0.03679 5.321 1.46e-07 *** ## subsidy 0.20467 0.04291 4.769 2.33e-06 *** ## age -0.06558 0.01905 -3.443 0.000615 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.464 on 598 degrees of freedom ## (64 observations deleted due to missingness) ## Multiple R-squared: 0.05574, Adjusted R-squared: 0.05258 ## F-statistic: 17.65 on 2 and 598 DF, p-value: 3.565e-08 … As well as multiple imputation. library(mice) set.seed(1) imp &lt;- mice(d) fit &lt;- with(imp, lm(act ~ subsidy + age)) pool(fit) |&gt; summary() ## term estimate std.error statistic df p.value ## 1 (Intercept) 0.18965550 0.03496624 5.423960 550.9040 8.739647e-08 ## 2 subsidy 0.19910361 0.04072650 4.888798 580.3649 1.314949e-06 ## 3 age -0.06753163 0.01892160 -3.569022 153.9652 4.781627e-04 In all cases, the point estimate for both the intercept and effect of subsidy is around 0.20. 6.2 “Combine then predict” or “predict then combine”? In the main text we say that we have a choice to make when working with a model fitted on multiply imputed data: Do we apply Rubin’s pooling rules on the model coefficients or on model predictions? We can refer to these different approaches as “combine then predict” and “predict then combine,” respectively, following Miles (2016). Since different R packages implement post-processing of multiply imputed model fits differently, we show a basic implementations here with two popular R packages. What the packages have in common is that they allow us to conveniently work with the mice object as we would any other model fit. Let’s proceed with the malaria subsidy data and imagine we want an average treatment effect of the subsidy treatment of ACT uptake marginal of age using g-computation. 6.2.1 marginaleffects The marginaleffects package (Arel-Bundock, Greifer, and Heiss Forthcoming) implements “predict then combine”; that is, it obtains model predictions for each of the m data sets and then apply Rubin’s rules to the predictions to pool them. G-computation with multiply imputed datasets is very straightforward to implement. library(marginaleffects) avg_comparisons(fit, variables = list(subsidy = 0:1)) ## ## Estimate Std. Error t Pr(&gt;|t|) S 2.5 % 97.5 % Df ## 0.199 0.0407 4.89 &lt;0.001 19.9 0.119 0.279 5965 ## ## Term: subsidy ## Type: response ## Comparison: mean(1) - mean(0) ## Columns: term, contrast, estimate, std.error, s.value, predicted_lo, predicted_hi, predicted, df, statistic, p.value, conf.low, conf.high 6.2.2 emmeans On the other hand, the emmeans package (R. V. Lenth 2024) implements “combine then predict”; that is, it applies Rubin’s rules to the model coefficients to pool them and only then obtain model predictions. A basic g-computation implementation with multiply imputed datasets could like this using emmeans. library(emmeans) emmeans(specs = &quot;subsidy&quot;, ref_grid(fit)) |&gt; contrast(&quot;revpairwise&quot;) |&gt; confint() ## contrast estimate SE df lower.CL upper.CL ## subsidy1 - subsidy0 0.199 0.0407 580 0.119 0.279 ## ## Confidence level used: 0.95 We see that results are identical to those obtained with marginaleffects. 6.2.3 Comparison But let’s try to see when the two different approaches give different results, such as for non-linear models. Instead of a linear model on the imputed datasets, we instead use a non-linear model in the form of a logistic regression. We do this by calling glm() instead of lm() and setting family = \"binomial\". fit_binom &lt;- with(imp, glm(act ~ subsidy + age, family = &quot;binomial&quot;)) Then, we apply our two approaches. The marginaleffects implementation is identical to above even though the model fit is now a logistic regression. avg_comparisons(fit_binom, variables = list(subsidy = 0:1)) ## ## Estimate Std. Error t Pr(&gt;|t|) S 2.5 % 97.5 % Df ## 0.201 0.0369 5.44 &lt;0.001 24.1 0.128 0.273 3541 ## ## Term: subsidy ## Type: response ## Comparison: mean(1) - mean(0) ## Columns: term, contrast, estimate, std.error, s.value, predicted_lo, predicted_hi, predicted, df, statistic, p.value, conf.low, conf.high For the emmeans approach, to get predictions on the probability scale, we can wrap the ref_grid() function in regrid() and set type = \"response\". We see that results are very similar but not identical. emmeans(specs = &quot;subsidy&quot;, regrid(ref_grid(fit_binom), type = &quot;response&quot;)) |&gt; contrast(&quot;revpairwise&quot;) |&gt; confint() ## contrast estimate SE df lower.CL upper.CL ## subsidy1 - subsidy0 0.203 0.037 1566 0.13 0.275 ## ## Confidence level used: 0.95 6.3 Bayesian imputation In the book, we also mentioned an alternative imputation approach, namely Bayesian imputation. Using the brms package (Bürkner 2017, 2018, 2021), we’ll show a very basic implementation. The syntax should seem somewhat familiar, as brms uses common R regression syntax. First, we define the model formula. At its core, it’s similar to the lm() formula above, except for a few complications. The formula object holds two main components, each wrapped by bf(). In the first part, we indicate the covariate we want to impute – in the case, age – with the mi() wrapper. The second part specifies a model for the variable we want to impute. Finally, we feed that formula to brm() along with the data. We then set the cores to 4 for speedier sampling and a seed for numeric reproducibility. We strongly recommend McElreath (and Kurz’ translation) (Kurz 2023; McElreath 2020) for more details and a general introduction to Bayesian inference more generally. library(brms) # Define model formula formula &lt;- bf(act ~ subsidy + mi(age)) + bf(age | mi() ~ 1) # Fit model to data bfit &lt;- brm(formula, data = d, cores = 4, seed = 2020, file = &quot;fits/bfit.rds&quot;) We can get a summary of the Bayesian model by calling summary() summary(bfit) ## Family: MV(gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: act ~ subsidy + mi(age) ## age | mi() ~ 1 ## Data: d (Number of observations: 631) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Regression Coefficients: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## act_Intercept 0.19 0.04 0.12 0.26 1.00 8466 2986 ## age_Intercept -0.02 0.04 -0.10 0.06 1.00 6946 3032 ## act_subsidy 0.20 0.04 0.12 0.28 1.00 7634 2840 ## act_miage -0.06 0.02 -0.10 -0.03 1.00 9493 2986 ## ## Further Distributional Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_act 0.46 0.01 0.44 0.49 1.00 8351 3073 ## sigma_age 1.00 0.03 0.94 1.06 1.00 6670 2543 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The coefficient act_subsidy corresponds to the subsidy coefficient in the lm() and glm() calls above. We see that the coefficients are very similar to the models above, except now we also have coefficients for the model predicting age (the age_ parameters). Now, there are many ways to extend this model, which are outside the scope of this companion website, for instance using more informative prior settings, constraining imputed values to be within realistic ranges, include covariates in the sub-model predicting missing age values, or use a different likelihood for act that respects its binary nature (e.g., a logistic regression model). We can also inspect the distribution of imputed values. For instance, we can plot 20 draws from the distribution of the imputed age variable (light blue curves) against the observed distribution of age in the sample (dark blue curve)… pp_check(bfit, resp = &quot;age&quot;, ndraws = 20) … Which is somewhat similar to this plot of imputed data sets (red curves) against the observed (blue curve) from the mice implementation shown in the text. densityplot(imp, ~ age) Now, we can post-process the Bayesian model fit exactly as shown in Chapters 3 and 5 using the tidybayes-based workflow (Kay 2023). Or, we can use marginaleffects just as shown for the mice object above, except we need to specify that it’s the model predicting ACT uptake that we want to apply g-computation to; this is what resp = \"act\" does. avg_comparisons(bfit, variables = list(subsidy = 0:1), resp = &quot;act&quot;) ## ## Estimate 2.5 % 97.5 % ## 0.195 0.116 0.276 ## ## Term: subsidy ## Type: response ## Comparison: mean(1) - mean(0) ## Columns: term, contrast, estimate, conf.low, conf.high, predicted_lo, predicted_hi, predicted, tmp_idx 6.4 Session info sessionInfo() ## R version 4.4.1 (2024-06-14 ucrt) ## Platform: x86_64-w64-mingw32/x64 ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=Danish_Denmark.utf8 LC_CTYPE=Danish_Denmark.utf8 ## [3] LC_MONETARY=Danish_Denmark.utf8 LC_NUMERIC=C ## [5] LC_TIME=Danish_Denmark.utf8 ## ## time zone: Europe/Copenhagen ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] emmeans_1.10.5 mice_3.16.0 haven_2.5.4 ## [4] cobalt_4.5.5 causaldata_0.1.4 marginaleffects_0.23.0 ## [7] dplyr_1.1.4 tidybayes_3.0.6 brms_2.21.0 ## [10] Rcpp_1.0.12 sandwich_3.1-1 boot_1.3-30 ## [13] patchwork_1.3.0 ggplot2_3.5.1 ## ## loaded via a namespace (and not attached): ## [1] gridExtra_2.3 inline_0.3.19 rlang_1.1.4 ## [4] magrittr_2.0.3 matrixStats_1.3.0 compiler_4.4.1 ## [7] mgcv_1.9-1 loo_2.8.0 vctrs_0.6.5 ## [10] reshape2_1.4.4 stringr_1.5.1 pkgconfig_2.0.3 ## [13] shape_1.4.6.1 arrayhelpers_1.1-0 crayon_1.5.3 ## [16] fastmap_1.2.0 backports_1.5.0 labeling_0.4.3 ## [19] rmarkdown_2.27 tzdb_0.4.0 nloptr_2.1.1 ## [22] purrr_1.0.2 xfun_0.48 glmnet_4.1-8 ## [25] jomo_2.7-6 cachem_1.1.0 jsonlite_1.8.8 ## [28] collapse_2.0.16 highr_0.11 pan_1.9 ## [31] chk_0.9.2 broom_1.0.6 parallel_4.4.1 ## [34] R6_2.5.1 bslib_0.7.0 stringi_1.8.4 ## [37] StanHeaders_2.35.0.9000 rpart_4.1.23 jquerylib_0.1.4 ## [40] estimability_1.5.1 bookdown_0.41 rstan_2.35.0.9000 ## [43] iterators_1.0.14 knitr_1.47 zoo_1.8-12 ## [46] readr_2.1.5 bayesplot_1.11.1 nnet_7.3-19 ## [49] Matrix_1.7-0 splines_4.4.1 tidyselect_1.2.1 ## [52] rstudioapi_0.16.0 abind_1.4-8 yaml_2.3.8 ## [55] codetools_0.2-20 pkgbuild_1.4.4 lattice_0.22-6 ## [58] tibble_3.2.1 plyr_1.8.9 withr_3.0.2 ## [61] bridgesampling_1.1-2 posterior_1.6.1 coda_0.19-4.1 ## [64] evaluate_1.0.4 survival_3.6-4 RcppParallel_5.1.7 ## [67] ggdist_3.3.2 pillar_1.10.2 tensorA_0.36.2.1 ## [70] checkmate_2.3.1 foreach_1.5.2 stats4_4.4.1 ## [73] insight_0.20.5 distributional_0.5.0 generics_0.1.4 ## [76] hms_1.1.3 rstantools_2.4.0 munsell_0.5.1 ## [79] scales_1.3.0 minqa_1.2.7 xtable_1.8-4 ## [82] glue_1.7.0 tools_4.4.1 data.table_1.15.4 ## [85] lme4_1.1-35.4 forcats_1.0.0 mvtnorm_1.2-5 ## [88] grid_4.4.1 tidyr_1.3.1 QuickJSR_1.2.2 ## [91] colorspace_2.1-0 nlme_3.1-164 cli_3.6.2 ## [94] svUnit_1.0.6 Brobdingnag_1.2-9 gtable_0.3.5 ## [97] sass_0.4.9 digest_0.6.35 farver_2.1.2 ## [100] htmltools_0.5.8.1 lifecycle_1.0.4 mitml_0.4-5 ## [103] MASS_7.3-60.2 References "],["chapter-7.html", "Chapter 7 Multilevel Modelling and Mundlak’s Legacy 7.1 Multilevel malaria medicine 7.2 Simulating Mundlak 7.3 Education and prosociality: Mundlak in action 7.4 Marginal effects in a multilevel model 7.5 Session info", " Chapter 7 Multilevel Modelling and Mundlak’s Legacy library(brms) library(ggplot2) library(patchwork) library(dplyr) library(tidybayes) 7.1 Multilevel malaria medicine To reproduce the figure comparing random and fixed effects, we first load the malaria subsidy data and fit the five models. We again use the brms package with default prior settings, but models could just as well be implemented using e.g., lme4. d &lt;- read.csv(&quot;data/cohen2015.csv&quot;) d$stratum &lt;- as.factor(d$stratum) 7.1.1 Naïve model: Simple intercept and slope msimp &lt;- brm(act ~ 1 + subsidy, data = d, cores = 4, seed = 1, file = &quot;fits/msimp&quot;) 7.1.2 Fixed effects mfix &lt;- brm(act ~ 1 + subsidy + stratum, data = d, cores = 4, seed = 1, file = &quot;fits/mfix.rds&quot;) 7.1.3 Fixed effects interacting treatment and group mfix2 &lt;- brm(act ~ 1 + subsidy * stratum, data = d, cores = 4, seed = 1, file = &quot;fits/mfix2.rds&quot;) 7.1.4 Random intercepts mran &lt;- brm(act ~ 1 + subsidy + (1 | stratum), data = d, cores = 4, seed = 1, file = &quot;fits/mran.rds&quot;) 7.1.5 Random intercepts and slopes mran2 &lt;- brm(act ~ 1 + subsidy + (1 + subsidy | stratum), data = d, cores = 4, seed = 1, file = &quot;fits/mran2.rds&quot;) 7.1.6 Plot models in a panel First, we extract and collect model coefficients in a data frame. FEREpanel &lt;- data.frame(intercept = c(fixef(msimp)[1], fixef(mran)[1], coef(mran)$stratum[,1,1], fixef(mran2)[1], coef(mran2)$stratum[,1,1], fixef(mfix)[1], fixef(mfix)[3:29] + fixef(mfix)[1], fixef(mfix2)[1], fixef(mfix2)[3:29] + fixef(mfix2)[1]), slope = c(fixef(msimp)[1], fixef(mran)[2], coef(mran)$stratum[,1,2], fixef(mran2)[2], coef(mran2)$stratum[,1,2], rep(fixef(mfix)[2],28), fixef(mfix2)[2], fixef(mfix2)[30:56] + fixef(mfix2)[2]), g = c(1, rep(1:29, 2), rep(1:28, 2)), grand = c(1, 1, rep(0,28), 1, rep(0,28), rep(0, 56)), model = c(&quot;Y ~ 1 + X&quot;, rep(&quot;Y ~ 1 + X + (1 | G)&quot;, 29), rep(&quot;Y ~ 1 + X + (1 + X | G)&quot;, 29), rep(&quot;Y ~ 1 + X + G&quot;, 28), rep(&quot;Y ~ 1 + X * G&quot;, 28))) Next, we generate predictions from each of the models. # Define a range of x-values (here, it&#39;s simply control (X = 0) vs. treatment (X = 1) x_values &lt;- seq(0, 1, by = 1) # Expand the dataframe to include x-values for each line... FEREpanel &lt;- FEREpanel |&gt; tidyr::expand_grid(x = x_values) |&gt; # ... and compute predictions for control vs. treatment dplyr::mutate(y = intercept + slope * x) Then we plot predictions from each of the models # Set y-axis limit ylim &lt;- c(0,1) # Generate individual plots p1 &lt;- FEREpanel |&gt; filter(model == &quot;Y ~ 1 + X&quot;) |&gt; ggplot(aes(x = x, y = y)) + geom_line() + theme_classic() + labs(subtitle = &quot;Simple intercept and slope&quot;, title = &quot;Y ~ 1 + X&quot;, y = NULL, x = NULL) + scale_x_continuous(breaks = c(0,1), labels = c(&quot;Control&quot;, &quot;Treatment&quot;), expand = c(0.1, 0.1)) + coord_cartesian(ylim = ylim) p2 &lt;- FEREpanel |&gt; filter(model == &quot;Y ~ 1 + X + G&quot;) |&gt; ggplot(aes(x = x, y = y, group = g)) + geom_line(alpha = 0.15) + theme_classic() + theme(legend.position = &quot;none&quot;) + labs(subtitle = &quot;Fixed effects&quot;, title = &quot;Y ~ 1 + X + G&quot;, y = &quot;Prob. of taking ACT&quot;, x = NULL) + scale_x_continuous(breaks = c(0,1), labels = c(&quot;Control&quot;, &quot;Treatment&quot;), expand = c(0.1, 0.1)) + coord_cartesian(ylim = ylim) p3 &lt;- FEREpanel |&gt; filter(model == &quot;Y ~ 1 + X * G&quot;) |&gt; ggplot(aes(x = x, y = y, group = g)) + geom_line(alpha = 0.15) + theme_classic() + theme(legend.position = &quot;none&quot;, axis.ticks.y = element_blank(), axis.text.y = element_blank()) + labs(subtitle = &quot;FE interacting X and group&quot;, title = &quot;Y ~ 1 + X * G&quot;, y = NULL, x = NULL) + scale_x_continuous(breaks = c(0,1), labels = c(&quot;Control&quot;, &quot;Treatment&quot;), expand = c(0.1, 0.1)) + coord_cartesian(ylim = ylim) p4 &lt;- FEREpanel |&gt; filter(model == &quot;Y ~ 1 + X + (1 | G)&quot;) |&gt; ggplot(aes(x = x, y = y, group = g, alpha = factor(grand))) + geom_line() + scale_alpha_manual(values = c(0.15, 1)) + theme_classic() + theme(legend.position = &quot;none&quot;) + labs(subtitle = &quot;Random intercepts&quot;, title = &quot;Y ~ 1 + X + (1 | G)&quot;, y = NULL, x = NULL) + scale_x_continuous(breaks = c(0,1), labels = c(&quot;Control&quot;, &quot;Treatment&quot;), expand = c(0.1, 0.1)) + coord_cartesian(ylim = ylim) p5 &lt;- FEREpanel |&gt; filter(model == &quot;Y ~ 1 + X + (1 + X | G)&quot;) |&gt; ggplot(aes(x = x, y = y, group = g, alpha = factor(grand))) + geom_line() + scale_alpha_manual(values = c(0.15, 1)) + theme_classic() + theme(legend.position = &quot;none&quot;, axis.ticks.y = element_blank(), axis.text.y = element_blank()) + labs(subtitle = &quot;Random intercepts and slopes&quot;, title = &quot;Y ~ 1 + X + (1 + X | G)&quot;, y = NULL, x = NULL) + scale_x_continuous(breaks = c(0,1), labels = c(&quot;Control&quot;, &quot;Treatment&quot;), expand = c(0.1, 0.1)) + coord_cartesian(ylim = ylim) Finally, we panel the individual plots using the patchwork package layout &lt;- &quot; A# BC DE &quot; (p1 + p2 + p3 + p4 + p5 + plot_layout(design = layout)) 7.1.7 Regularized vs. empirical estimates In the text, we show an alternative way to demonstrate partial pooling in a multilevel model. This is where we compare the regularized predictions from a multilevel model against the “empirical” estimates from a fixed effects model. We re-use the model objects from above, mfix2 and mran2, where the treatment effect is allowed to vary by strata. First, we create two data frames that include the estimated treatment effect for each randomization stratum for each of the two models. # Fixed effects treatment effects coeffix &lt;- data.frame( strata = 1:28, coef = fixef(mfix2)[c(2, 30:56), 1]) coeffix$coef[2:28] &lt;- coeffix$coef[2:28] + coeffix$coef[1] # Random effects treatment effects coefran &lt;- data.frame( strata = 1:28, coef = coef(mran2)$stratum[,&quot;Estimate&quot;, &quot;subsidy&quot;]) # Compute strata sample sizes N &lt;- d |&gt; group_by(stratum) |&gt; summarise(N = n()) # Input strata sample sizes in data frames coeffix$N &lt;- N$N coefran$N &lt;- N$N # Arrange data frames according to strata sample sizes coeffix &lt;- coeffix |&gt; arrange(N) coeffix$strata &lt;- factor(coeffix$strata, levels = coeffix$strata) coefran &lt;- coefran |&gt; arrange(N) coefran$strata &lt;- factor(coefran$strata, levels = coefran$strata) Then, we plot the stratum-specific treatment effects. ggplot() + geom_line(data = rbind(coeffix, coefran), aes(x = strata, y = coef, group = strata), linewidth = 0.25, alpha = 0.5) + geom_point(data = coeffix, aes(x = strata, y = coef), size = 1.5, alpha = 0.8, shape = 1) + geom_point(data = coefran, aes(x = strata, y = coef), size = 1.5, color = &quot;black&quot;) + geom_hline(yintercept = 0.19, linetype = &quot;dashed&quot;, alpha = 0.2, linewidth = 0.5) + geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = 0.07, ymax = 0.3), fill = &quot;lightgrey&quot;, alpha = 0.2) + ylab(&quot;Treatment Effect&quot;) + xlab(&quot;Randomization Strata\\n(smallest to largest)&quot;) + theme_classic() + theme(axis.text.x = element_blank(), axis.ticks = element_blank()) Instead of using a fixed effects model, an alternative way to compute the “empirical” estimates would be to, within each stratum, take the mean of the outcome variable act for the control (subsidy = 0) and treatment (subsidy = 1) groups separately and then subtract those values. d |&gt; # within each stratum and for each subsidy condition (control vs. treatment)... group_by(stratum, subsidy) |&gt; # ... take the mean of the outcome variable summarise(&quot;T&quot; = mean(act, na.rm = T)) |&gt; # wrangle and name columns tidyr::pivot_wider(names_from = &quot;subsidy&quot;, values_from = &quot;T&quot;, names_prefix = &quot;T&quot;) |&gt; ungroup() |&gt; mutate(coef = T1 - T0) 7.2 Simulating Mundlak Here we show the simulation and plotting code for the synthetic Mundlak demonstration. First, we simulate the confounded data and store it in d_sim set.seed(2025) # define sample size and effects N_groups &lt;- 30 N_id &lt;- 500 a &lt;- 0 bZY &lt;- 1 bXY &lt;- 0.5 g &lt;- sample(1:N_groups, size = N_id, replace = TRUE) # sample into groups Ug &lt;- rnorm(N_groups, 1.5, 1) # group confounds X &lt;- rnorm(N_id, Ug[g], 1) # individual varying trait Z &lt;- rnorm(N_groups, 0, 1) # group varying trait (observed) Y &lt;- rnorm(N_id, a + bXY*X + Ug[g] + bZY*Z[g] ) # collect in data frame d_sim &lt;- data.frame( Y = Y, X = X, Z = Z[g], G = as.factor(g) ) To properly quantify uncertainty and obtain neat posterior distributions of the effects from each model, we again analyze the data in a Bayesian framework using brms with default priors. We fit our four models. 7.2.1 Naïve model (ignoring group) mNA &lt;- brm(Y ~ X + Z, cores = 4, data = d_sim, seed = 1, file = &quot;fits/mNA.rds&quot;) 7.2.2 Fixed effects mFE &lt;- brm(Y ~ X + G, cores = 4, data = d_sim, seed = 1, file = &quot;fits/mFE.rds&quot;) 7.2.3 Random intercepts mRE &lt;- brm(Y ~ X + Z + (1 | G), cores = 4, data = d_sim, seed = 1, file = &quot;fits/mRE.rds&quot;) 7.2.4 Mundlak model d_sim$Xbar &lt;- with(d_sim, ave(X, G, FUN = mean)) mMU &lt;- brm(Y ~ X + Xbar + Z + (1 | G), cores = 4, data = d_sim, seed = 1, file = &quot;fits/mMU.rds&quot;) 7.2.5 Plot effect estimate distributions First, we extract posterior distributions of the coefficient of interest (X) from each model and collect in a data frame. ndraws &lt;- nrow(brms::as_draws_df(mNA, variable = &quot;b_X&quot;)) forest &lt;- data.frame(bX = c(brms::as_draws_df(mNA, variable = &quot;b_X&quot;)$b_X, brms::as_draws_df(mFE, variable = &quot;b_X&quot;)$b_X, brms::as_draws_df(mRE, variable = &quot;b_X&quot;)$b_X, brms::as_draws_df(mMU, variable = &quot;b_X&quot;)$b_X), model = c(rep(&quot;Naïve model\\nY ~ X + Z&quot;, ndraws), rep(&quot;Fixed effects\\nY ~ X + Z + G&quot;, ndraws), rep(&quot;Random effects\\nY ~ X + Z + (1 | G)&quot;, ndraws), rep(&quot;Mundlak model\\nY ~ X + Xbar + Z + (1 | G)&quot;, ndraws))) Next, the models are arranged… forest$model &lt;- factor(forest$model, levels=c(&quot;Naïve model\\nY ~ X + Z&quot;, &quot;Fixed effects\\nY ~ X + Z + G&quot;, &quot;Random effects\\nY ~ X + Z + (1 | G)&quot;, &quot;Mundlak model\\nY ~ X + Xbar + Z + (1 | G)&quot;)) |&gt; forcats::fct_rev() … and then plotted. forest |&gt; ggplot(aes(y = model, x = bX)) + stat_halfeye(slab_fill = &quot;white&quot;, slab_color = &quot;grey40&quot;, color = &quot;white&quot;) + geom_vline(xintercept = 0.5, linetype = &quot;dashed&quot;, alpha = 0.5) + labs(y = NULL, x = &quot;Regression coefficient&quot;) + scale_x_continuous(breaks = c(0.25, 0.5, 0.75, 1)) + theme_classic() 7.3 Education and prosociality: Mundlak in action We finally show the Mundlak model in action in real-world data. We load the cerc data (Martin Lang et al. 2019) and plot the raw data distribution for the key predictor and outcome variable, respectively, before fitting and plotting our four models. cerc &lt;- read.csv(&quot;data/cerc.csv&quot;) 7.3.1 Raw data distributions cerc |&gt; ggplot(aes(x = FORMALED)) + geom_density(aes(y = after_stat(scaled))) + facet_wrap(~ SITE, nrow = 2) + labs(y = NULL, x = NULL) + scale_x_continuous(n.breaks = 3) + scale_y_continuous(breaks = NULL) + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) + theme_classic() + ggtitle(&quot;Years of formal education&quot;) cerc |&gt; ggplot(aes(x = Y)) + geom_density(aes(y = after_stat(scaled))) + facet_wrap(~ SITE, nrow = 2) + labs(y = NULL, x = NULL) + scale_x_continuous(n.breaks = 3) + scale_y_continuous(breaks = NULL) + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) + theme_classic() + ggtitle(&quot;Coins to co-player&quot;) 7.3.2 Naïve model mNAcerc &lt;- brm(Y ~ 1 + FORMALED, data = cerc, cores = 4, seed = 1, file = &quot;fits/mNAcerc.rds&quot;) 7.3.3 Fixed effects mFEcerc &lt;- brm(Y ~ 1 + FORMALED+ SITE, data = cerc, cores = 4, seed = 1, file = &quot;fits/mFEcerc.rds&quot;) 7.3.4 Random effects mREcerc &lt;- brm(Y ~ 1 + FORMALED + (1 | SITE), data = cerc, cores = 4, seed = 1, file = &quot;fits/mREcerc.rds&quot;) 7.3.5 Mundlak model cerc$Xbar &lt;- with(cerc, ave(FORMALED, SITE, FUN = mean)) mMUcerc &lt;- brm(Y ~ 1 + FORMALED + Xbar + (1 | SITE), data = cerc, cores = 4, seed = 1, file = &quot;fits/mMUcerc.rds&quot;) 7.3.6 Plotting effect of education on religiosity The code for this plot is much the same as for the simulated Mundlak example above. ndraws &lt;- nrow(brms::as_draws_df(mNAcerc, variable = &quot;b_FORMALED&quot;)) forestcerc &lt;- data.frame(bX = c(brms::as_draws_df(mNAcerc, variable = &quot;b_FORMALED&quot;)$b_FORMALED, brms::as_draws_df(mFEcerc, variable = &quot;b_FORMALED&quot;)$b_FORMALED, brms::as_draws_df(mREcerc, variable = &quot;b_FORMALED&quot;)$b_FORMALED, brms::as_draws_df(mMUcerc, variable = &quot;b_FORMALED&quot;)$b_FORMALED), model = c(rep(&quot;Naïve model&quot;, ndraws), rep(&quot;Fixed effects&quot;, ndraws), rep(&quot;Random effects&quot;, ndraws), rep(&quot;Mundlak model&quot;, ndraws))) forestcerc$model &lt;- factor(forestcerc$model, levels=c(&quot;Naïve model&quot;, &quot;Fixed effects&quot;, &quot;Random effects&quot;, &quot;Mundlak model&quot;)) |&gt; forcats::fct_rev() forestcerc |&gt; ggplot(aes(y = model, x = bX)) + stat_halfeye(point_interval = &quot;mean_hdci&quot;, .width = 0.95, slab_fill = &quot;white&quot;, slab_color = &quot;grey40&quot;, color = &quot;white&quot;) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;, alpha = 0.5) + labs(title = &quot;Cross-cultural Dictator Game&quot;, y = NULL, x = &quot;Effect of Education on Prosociality&quot;) + theme_classic() 7.4 Marginal effects in a multilevel model As we discuss in the text, there are several ways of obtaining predictions from a multilevel model, including predictions for each site, for an average site and marginal of site. Note that we here show a general g-computation approach, where we first compute marginal effects within each MCMC draw (group_by(.draw, ...) |&gt; summarise(.epred = mean(.epred))), before summarizing by the posterior mean and quantile intervals across MCMC draws (group_by(FORMALED) |&gt; summarise(.epred = mean_qi(.epred))). In the current data example, the group_by(.draw, ...) step is redundant, but it’s how we would go about applying g-computation to obtain marginal effects in a setting with a continuous focal (causal) predictor and with possible covariates and nonlinearities. First, we fit an extended Mundlak model, that allows the effect of education on religiosity to vary by site. mMUcerc2 &lt;- brm(Y ~ 1 + FORMALED + Xbar + (1 + FORMALED | SITE), data = cerc, cores = 4, seed = 1, file = &quot;fits/mMUcerc2.rds&quot;) 7.4.1 Predicting the observed sites We can then visualize effect estimates for each group (field site, in this case) across the full range of observed education years and at site-specific average years of education. # prepare prediction grid across full range of education years... nd &lt;- tidyr::expand_grid(FORMALED = c(0,10,20,30), SITE = unique(cerc$SITE)) # ... and at site-specific average years of education nd$Xbar &lt;- rep(unique(cerc$Xbar), length(unique(nd$FORMALED))) Plot predictions for each site in separate panels. For this, we need to include the random effects in the predictions. # Predict effect estimates... add_epred_draws(mMUcerc2, # for prediction grid and... newdata = nd, # ... *include* all random effect components. re_formula = NULL) |&gt; # Compute average effect *within* each MCMC draw. group_by(.draw, SITE, FORMALED) |&gt; summarise(.epred = mean(.epred)) |&gt; # Summarise average effect *across* each MCMC draw # for each site and educational level. group_by(SITE, FORMALED) |&gt; summarise(mean_qi(.epred)) |&gt; # Plot! ggplot(aes(x = FORMALED, y = y, ymin = ymin, ymax = ymax)) + geom_lineribbon(color = &quot;blue&quot;, fill = &quot;grey90&quot;, linewidth = 0.5) + coord_cartesian(ylim = c(0,8)) + facet_wrap(~SITE, nrow = 2) + theme_classic() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Predicting the observed sites&quot;, subtitle = &quot;Including random effects&quot;, x = &quot;Years of education&quot;, y = &quot;Coins to co-player&quot;) But we can also ignore site-specific effects altogether and instead aim at predicting a perfectly average site. This amounts to ignoring the random effects of the model. We first need to set up a new prediction grid for the new hypothetical site – let’s call it “Newland” – with an average of 7 years of education, the average of the site averages (this can be checked by running mean(unique(cerc$Xbar))). nd2 &lt;- tidyr::expand_grid(FORMALED = c(0,10,20,30), SITE = &quot;Newland&quot;, # could also just set to NA Xbar = 7) # could also make a distribution of Xbars to average over # Predict effect estimates... p1 &lt;- add_epred_draws(mMUcerc2, # for new prediction grid and... newdata = nd2, # ... *ignore* all random effect components. re_formula = NA) |&gt; # Compute average effect *within* each MCMC draw. group_by(.draw, FORMALED) |&gt; summarise(.epred = mean(.epred)) |&gt; # Summarise average effect *across* MCMC draws # for each educational level. group_by(FORMALED) |&gt; summarise(mean_qi(.epred)) |&gt; # Plot! ggplot(aes(x = FORMALED, y = y, ymin = ymin, ymax = ymax)) + geom_lineribbon(color = &quot;blue&quot;, fill = &quot;grey90&quot;, linewidth = 0.5) + coord_cartesian(ylim = c(0,8)) + scale_fill_brewer() + theme_classic() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Predicting the average site&quot;, subtitle = &quot;Ignoring random effects&quot;, x = &quot;Years of education&quot;, y = &quot;Coins to co-player&quot;) And finally, we could be interested in predicting a new site drawing from all that we know about the observed sites. This amounts to averaging over – instead of ignoring, as above – the uncertainty that we have around the observed sites and generating predictions from that. # Predict effect estimates... p2 &lt;- add_epred_draws(mMUcerc2, # for prediction grid and... newdata = nd2, # ... *include* all random effect components. re_formula = NULL, # Allow predictions for an unobserved group and... allow_new_levels = TRUE, # ... sample from the implied multivariate gaussian. sample_new_levels = &quot;gaussian&quot;) |&gt; # Compute average effect *within* each MCMC draw. group_by(.draw, FORMALED) |&gt; summarise(.epred = mean(.epred)) |&gt; # Summarise average causal effect across MCMC draws # for each educational level. group_by(FORMALED) |&gt; summarise(mean_qi(.epred)) |&gt; # Plot! ggplot(aes(x = FORMALED, y = y, ymin = ymin, ymax = ymax)) + geom_lineribbon(color = &quot;blue&quot;, fill = &quot;grey90&quot;, linewidth = 0.5) + coord_cartesian(ylim = c(0,8)) + scale_fill_brewer() + theme_classic() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Predicting a new site&quot;, subtitle = &quot;Averaging over random effects&quot;, x = &quot;Years of education&quot;, y = NULL) + scale_y_continuous(breaks = NULL) p1 + p2 + plot_layout(axis_titles = &quot;collect&quot;) 7.4.2 Frequentist workflow For completeness, we can also plot predictions for each site with a frequentist approach. First, we fit a corresponding frequentist model. library(lme4) mMUcerc2_freq &lt;- lmer(Y ~ 1 + FORMALED + Xbar + (1 + FORMALED | SITE), data = cerc) Next, we obtain predictions using the marginaleffects package to obtain confidence intervals. Now, the frequentist approach excludes uncertainty in the random effects, so it’s not entirely comparable to the predictions obtained in the text. library(marginaleffects) # Predicting the observed sites (only with uncertainty in the global/fixed effects parameters) predictions(mMUcerc2_freq, newdata = nd) |&gt; ggplot(aes(x = FORMALED, y = estimate, ymin = conf.low, ymax = conf.high)) + geom_lineribbon(color = &quot;blue&quot;, fill = &quot;grey90&quot;, linewidth = 0.5) + coord_cartesian(ylim = c(0,8)) + facet_wrap(~SITE, nrow = 2) + theme_classic() + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Predicting the observed sites&quot;, subtitle = &quot;Frequentist version&quot;, x = &quot;Years of education&quot;, y = &quot;Coins to co-player&quot;) 7.5 Session info sessionInfo() ## R version 4.4.1 (2024-06-14 ucrt) ## Platform: x86_64-w64-mingw32/x64 ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=Danish_Denmark.utf8 LC_CTYPE=Danish_Denmark.utf8 ## [3] LC_MONETARY=Danish_Denmark.utf8 LC_NUMERIC=C ## [5] LC_TIME=Danish_Denmark.utf8 ## ## time zone: Europe/Copenhagen ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] lme4_1.1-35.4 Matrix_1.7-0 emmeans_1.10.5 ## [4] mice_3.16.0 haven_2.5.4 cobalt_4.5.5 ## [7] causaldata_0.1.4 marginaleffects_0.23.0 dplyr_1.1.4 ## [10] tidybayes_3.0.6 brms_2.21.0 Rcpp_1.0.12 ## [13] sandwich_3.1-1 boot_1.3-30 patchwork_1.3.0 ## [16] ggplot2_3.5.1 ## ## loaded via a namespace (and not attached): ## [1] gridExtra_2.3 inline_0.3.19 rlang_1.1.4 ## [4] magrittr_2.0.3 matrixStats_1.3.0 compiler_4.4.1 ## [7] mgcv_1.9-1 loo_2.8.0 vctrs_0.6.5 ## [10] reshape2_1.4.4 stringr_1.5.1 pkgconfig_2.0.3 ## [13] shape_1.4.6.1 arrayhelpers_1.1-0 crayon_1.5.3 ## [16] fastmap_1.2.0 backports_1.5.0 labeling_0.4.3 ## [19] utf8_1.2.4 rmarkdown_2.27 tzdb_0.4.0 ## [22] nloptr_2.1.1 purrr_1.0.2 xfun_0.48 ## [25] glmnet_4.1-8 jomo_2.7-6 cachem_1.1.0 ## [28] jsonlite_1.8.8 collapse_2.0.16 highr_0.11 ## [31] pan_1.9 chk_0.9.2 broom_1.0.6 ## [34] parallel_4.4.1 R6_2.5.1 bslib_0.7.0 ## [37] stringi_1.8.4 StanHeaders_2.35.0.9000 rpart_4.1.23 ## [40] jquerylib_0.1.4 estimability_1.5.1 bookdown_0.41 ## [43] rstan_2.35.0.9000 iterators_1.0.14 knitr_1.47 ## [46] zoo_1.8-12 readr_2.1.5 bayesplot_1.11.1 ## [49] nnet_7.3-19 splines_4.4.1 tidyselect_1.2.1 ## [52] rstudioapi_0.16.0 abind_1.4-8 yaml_2.3.8 ## [55] codetools_0.2-20 pkgbuild_1.4.4 lattice_0.22-6 ## [58] tibble_3.2.1 plyr_1.8.9 withr_3.0.2 ## [61] bridgesampling_1.1-2 posterior_1.6.1 coda_0.19-4.1 ## [64] evaluate_1.0.4 survival_3.6-4 RcppParallel_5.1.7 ## [67] ggdist_3.3.2 pillar_1.10.2 tensorA_0.36.2.1 ## [70] checkmate_2.3.1 foreach_1.5.2 stats4_4.4.1 ## [73] insight_0.20.5 distributional_0.5.0 generics_0.1.4 ## [76] hms_1.1.3 rstantools_2.4.0 munsell_0.5.1 ## [79] scales_1.3.0 minqa_1.2.7 xtable_1.8-4 ## [82] glue_1.7.0 tools_4.4.1 data.table_1.15.4 ## [85] forcats_1.0.0 mvtnorm_1.2-5 grid_4.4.1 ## [88] tidyr_1.3.1 QuickJSR_1.2.2 colorspace_2.1-0 ## [91] nlme_3.1-164 cli_3.6.2 svUnit_1.0.6 ## [94] Brobdingnag_1.2-9 gtable_0.3.5 sass_0.4.9 ## [97] digest_0.6.35 farver_2.1.2 htmltools_0.5.8.1 ## [100] lifecycle_1.0.4 mitml_0.4-5 MASS_7.3-60.2 References "],["pkgs.html", "Chapter 8 Package Citations", " Chapter 8 Package Citations library(grateful) pkgs &lt;- cite_packages(output = &quot;table&quot;, out.dir = &quot;.&quot;, cite.tidyverse = TRUE, dependencies = TRUE) knitr::kable(pkgs) Package Version Citation abind 1.4.8 Plate and Heiberger (2024) arrayhelpers 1.1.0 Beleites (2020) backports 1.5.0 Michel Lang, Murdoch, and R Core Team (2024) base 4.4.1 R Core Team (2024) base64enc 0.1.3 Urbanek (2015) bayesplot 1.11.1 Gabry et al. (2019); Gabry and Mahr (2024) BH 1.84.0.0 Eddelbuettel, Emerson, and Kane (2024) bit 4.5.0 Oehlschlägel (2024a) bit64 4.5.2 Oehlschlägel (2024b) bookdown 0.41 Xie (2016); Xie (2024a) bridgesampling 1.1.2 Gronau, Singmann, and Wagenmakers (2020) brms 2.21.0 Bürkner (2017); Bürkner (2018); Bürkner (2021) Brobdingnag 1.2.9 Hankin (2007) bslib 0.7.0 Sievert, Cheng, and Aden-Buie (2024) cachem 1.1.0 Chang (2024a) callr 3.7.6 Csárdi and Chang (2024a) causaldata 0.1.4 Huntington-Klein and Barrett (2024) checkmate 2.3.1 Michel Lang (2017) chk 0.9.2 Thorley, Müller, and Pearson (2024) clipr 0.8.0 Lincoln (2022) cobalt 4.5.5 Greifer (2024) coda 0.19.4.1 Plummer et al. (2006) cpp11 0.4.7 Vaughan, Hester, and François (2023) crayon 1.5.3 Csárdi (2024) data.table 1.15.4 Barrett et al. (2024) desc 1.4.3 Csárdi, Müller, and Hester (2023) digest 0.6.35 Antoine Lucas et al. (2024) distributional 0.5.0 O’Hara-Wild et al. (2024) emmeans 1.10.5 R. V. Lenth (2024) estimability 1.5.1 R. Lenth (2024) evaluate 1.0.4 Wickham and Xie (2025) farver 2.1.2 Pedersen, Nicolae, and François (2024) fastmap 1.2.0 Chang (2024b) fontawesome 0.5.2 Iannone (2023) foreach 1.5.2 Microsoft and Weston (2022) Formula 1.2.5 Zeileis and Croissant (2010) fs 1.6.4 Hester, Wickham, and Csárdi (2024) future 1.33.2 Bengtsson (2021) future.apply 1.11.2 Bengtsson (2021) generics 0.1.4 Wickham, Kuhn, and Vaughan (2025) ggdist 3.3.2 Kay (2024b); Kay (2024a) ggridges 0.5.6 Wilke (2024) glmnet 4.1.8 Friedman, Tibshirani, and Hastie (2010); Simon et al. (2011); Tay, Narasimhan, and Hastie (2023) globals 0.16.3 Bengtsson (2024a) glue 1.7.0 Hester and Bryan (2024) gridExtra 2.3 Auguie (2017) gtable 0.3.5 Wickham and Pedersen (2024) highr 0.11 Xie and Qiu (2024) htmltools 0.5.8.1 Cheng, Sievert, et al. (2024) inline 0.3.19 Sklyar et al. (2021) insight 0.20.5 Lüdecke, Waggoner, and Makowski (2019) isoband 0.2.7 Wickham, Wilke, and Pedersen (2022) iterators 1.0.14 Analytics and Weston (2022) jomo 2.7.6 Quartagno and Carpenter (2023) jquerylib 0.1.4 Sievert and Cheng (2021) knitr 1.47 Xie (2014); Xie (2015); Xie (2024b) labeling 0.4.3 Justin Talbot (2023) lifecycle 1.0.4 Henry and Wickham (2023) listenv 0.9.1 Bengtsson (2024b) lme4 1.1.35.4 Bates et al. (2015) loo 2.8.0 Vehtari, Gelman, and Gabry (2017); Yao et al. (2017); Vehtari, Gabry, et al. (2024) marginaleffects 0.23.0 Arel-Bundock, Greifer, and Heiss (Forthcoming) matrixStats 1.3.0 Bengtsson (2024c) memoise 2.0.1 Wickham et al. (2021) mice 3.16.0 van Buuren and Groothuis-Oudshoorn (2011) mime 0.12 Xie (2021) minqa 1.2.7 Bates et al. (2024) mitml 0.4.5 Grund, Robitzsch, and Luedtke (2023) mvtnorm 1.2.5 Genz and Bretz (2009) nleqslv 3.3.5 Hasselman (2023) nloptr 2.1.1 S. G. Johnson (2008) numDeriv 2016.8.1.1 Gilbert and Varadhan (2019) ordinal 2023.12.4.1 Christensen (2023) pan 1.9 Zhao and Schafer (2023) parallelly 1.37.1 Bengtsson (2024d) patchwork 1.3.0 Pedersen (2024) pkgbuild 1.4.4 Wickham, Hester, and Csárdi (2024) pkgconfig 2.0.3 Csárdi (2019) plyr 1.8.9 Wickham (2011) posterior 1.6.1 Vehtari et al. (2021); Lambert and Vehtari (2022); Margossian et al. (2024); Vehtari, Simpson, et al. (2024); Bürkner et al. (2025) prettyunits 1.2.0 Csardi (2023) processx 3.8.4 Csárdi and Chang (2024b) progress 1.2.3 Csárdi and FitzJohn (2023) ps 1.7.6 Loden et al. (2024) quadprog 1.5.8 Berwin A. Turlach R port by Andreas Weingessel &lt;Andreas.Weingessel@ci.tuwien.ac.at&gt; Fortran contributions from Cleve Moler dpodi/LINPACK) (2019) QuickJSR 1.2.2 A. R. Johnson (2024) R6 2.5.1 Chang (2021) rappdirs 0.3.3 Ratnakumar, Mick, and Davis (2021) rbibutils 2.3 Boshnakov and Putman (2024) RColorBrewer 1.1.3 Neuwirth (2022) Rcpp 1.0.12 Eddelbuettel and François (2011); Eddelbuettel (2013); Eddelbuettel and Balamuta (2018); Eddelbuettel et al. (2024) RcppEigen 0.3.4.0.0 Bates and Eddelbuettel (2013) RcppParallel 5.1.7 Allaire et al. (2023) Rdpack 2.6.4 Boshnakov (2025) reformulas 0.4.1 Bolker (2025) remotes 2.5.0 Csárdi et al. (2024) renv 1.0.11 Ushey and Wickham (2024) reshape2 1.4.4 Wickham (2007) rmarkdown 2.27 Xie, Allaire, and Grolemund (2018); Xie, Dervieux, and Riederer (2020); Allaire et al. (2024) rstan 2.35.0.9000 Stan Development Team (n.d.) rstantools 2.4.0 Gabry et al. (2024) sandwich 3.1.1 Zeileis (2004); Zeileis (2006); Zeileis, Köll, and Graham (2020) sass 0.4.9 Cheng, Mastny, et al. (2024) scales 1.3.0 Wickham, Pedersen, and Seidel (2023) shape 1.4.6.1 Soetaert (2024) StanHeaders 2.35.0.9000 Stan Development Team (2020) stringi 1.8.4 Gagolewski (2022) svUnit 1.0.6 Grosjean (2024) tensorA 0.36.2.1 van den Boogaart (2023) tidybayes 3.0.6 Kay (2023) tidyselect 1.2.1 Henry and Wickham (2024) tidyverse 2.0.0 Wickham et al. (2019) tinytex 0.51 Xie (2019); Xie (2024c) tzdb 0.4.0 Vaughan (2023) ucminf 1.2.2 Nielsen and Mortensen (2024) utf8 1.2.4 Perry (2023) vctrs 0.6.5 Wickham, Henry, and Vaughan (2023) viridisLite 0.4.2 Garnier et al. (2023) vroom 1.6.5 Hester, Wickham, and Bryan (2023) withr 3.0.2 Hester et al. (2024) xfun 0.48 Xie (2024d) xtable 1.8.4 Dahl et al. (2019) yaml 2.3.8 Garbett et al. (2023) zoo 1.8.12 Zeileis and Grothendieck (2005) "]]
